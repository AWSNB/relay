{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The Sentry Relay is a work in progress service that pushes some functionality from the Sentry SDKs as well as the Sentry server into a proxy process. Getting started The Relay server is called relay . Binaries can be downloaded from the GitHub releases page . After downloading, place the binary somewhere on your PATH and make it executable. The config init command is provided to initialize the initial config. The config will be created the folder it's run from in a hidden .relay subdirectory: $ relay config init The wizard will ask a few questions: default vs custom config. In the default config the relay will connect to sentry.io for sending and will generally use the defaults. In the custom config a different upstream can be configured. Relay internal crash reporting can be enabled or disabled. When enabled the relay will report its own internal errors to sentry.io to help us debug it. Lastly the relay will ask it should run authenticated with credentials or not. Currently we do not yet support authenticated mode against sentry.io. You now have a folder named .relay in your current working directory. To launch the server, run: $ relay run If you moved your config folder somewhere else, you can use the --config option: $ relay run --config ./my/custom/relay_folder/ Running in Docker Docker image for relay can be found at us.gcr.io/sentryio/relay . For example, you can start the latest version of relay as follows: docker run -v $( pwd ) /configs/:/etc/relay/ us.gcr.io/sentryio/relay run --config /etc/relay The command assumes that Relay's configuration ( config.yml and credentials.json ) are stored in ./configs/ directory on the host machine. Upstream Registration When Relay runs, it registers with the upstream configured Sentry instance. Each Relay is identified by the (relay_id, public_key) tuple upstream. Multiple Relays can share the same public key if they run with different Relay IDs. At present, Sentry requires Relays to be explicitly whitelisted by their public key. This is done through the SENTRY_RELAY_WHITELIST_PK config key which is a list of permitted public keys. Metrics and Crash Reporting By default, Relay currently reports directly to sentry.io. This can be disabled by setting the sentry.enabled key to false . Additionally a different DSN can be supplied with sentry.dsn . Crash reporting will become opt-in before the initial public release. Stats can be submitted to a statsd server by configuring metrics.statsd key. It can be put to a ip:port tuple. Additionally metrics.prefix can be configured to have a different prefix (the default is sentry.relay ). This prefix is added in front of all metrics. Setting up a Project Right now Relay is only really usable in \"simple proxy mode\" (without credentials), and as such calls the same exact endpoints on Sentry that an SDK would. That also means you have to configure each project individually in Relay. Create a new file in the form project_id.json : .relay / projects / ___PROJECT_ID___.json With the following content: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ], \"config\" : { \"allowedDomains\" : [ \"*\" ], \"piiConfig\" : {} } } The relay has to know all public keys (i.e. the secret part of the DSN) that will send events to it. DSNs unknown for this project will be rejected. Sending a Test Event Launch the server with relay run , and set up any SDK with the following DSN: http : //___PUBLIC_KEY___@127.0.0.1:3000/___PROJECT_ID___ As you can see we only changed the host and port of the DSN to point to your Relay setup. You should be able to use the SDK normally at this point. Events arrive at the Sentry instance that Relay is configured to use in .relay/config.yml . PII Stripping Now let's get to the entire point of this proxy setup: Stripping sensitive data. The easiest way to go about this is if you already have a raw JSON payload from some SDK. Go to our PII config editor Piinguin , and: Paste in a raw event Click on data you want eliminated Paste in other payloads and see if they look fine, go to step 2 if necessary. After iterating on the config, paste it back into the project config you created earlier: .relay / projects / ___PROJECT_ID___.json For example: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ], \"config\" : { \"allowedDomains\" : [ \"*\" ], \"piiConfig\" : { \"rules\" : { \"device_id\" : { \"type\" : \"pattern\" , \"pattern\" : \"d/[a-f0-9]{12}\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"freeform\" : [ \"device_id\" ] } } } }","title":"Introduction"},{"location":"#introduction","text":"The Sentry Relay is a work in progress service that pushes some functionality from the Sentry SDKs as well as the Sentry server into a proxy process.","title":"Introduction"},{"location":"#getting-started","text":"The Relay server is called relay . Binaries can be downloaded from the GitHub releases page . After downloading, place the binary somewhere on your PATH and make it executable. The config init command is provided to initialize the initial config. The config will be created the folder it's run from in a hidden .relay subdirectory: $ relay config init The wizard will ask a few questions: default vs custom config. In the default config the relay will connect to sentry.io for sending and will generally use the defaults. In the custom config a different upstream can be configured. Relay internal crash reporting can be enabled or disabled. When enabled the relay will report its own internal errors to sentry.io to help us debug it. Lastly the relay will ask it should run authenticated with credentials or not. Currently we do not yet support authenticated mode against sentry.io. You now have a folder named .relay in your current working directory. To launch the server, run: $ relay run If you moved your config folder somewhere else, you can use the --config option: $ relay run --config ./my/custom/relay_folder/","title":"Getting started"},{"location":"#running-in-docker","text":"Docker image for relay can be found at us.gcr.io/sentryio/relay . For example, you can start the latest version of relay as follows: docker run -v $( pwd ) /configs/:/etc/relay/ us.gcr.io/sentryio/relay run --config /etc/relay The command assumes that Relay's configuration ( config.yml and credentials.json ) are stored in ./configs/ directory on the host machine.","title":"Running in Docker"},{"location":"#upstream-registration","text":"When Relay runs, it registers with the upstream configured Sentry instance. Each Relay is identified by the (relay_id, public_key) tuple upstream. Multiple Relays can share the same public key if they run with different Relay IDs. At present, Sentry requires Relays to be explicitly whitelisted by their public key. This is done through the SENTRY_RELAY_WHITELIST_PK config key which is a list of permitted public keys.","title":"Upstream Registration"},{"location":"#metrics-and-crash-reporting","text":"By default, Relay currently reports directly to sentry.io. This can be disabled by setting the sentry.enabled key to false . Additionally a different DSN can be supplied with sentry.dsn . Crash reporting will become opt-in before the initial public release. Stats can be submitted to a statsd server by configuring metrics.statsd key. It can be put to a ip:port tuple. Additionally metrics.prefix can be configured to have a different prefix (the default is sentry.relay ). This prefix is added in front of all metrics.","title":"Metrics and Crash Reporting"},{"location":"#setting-up-a-project","text":"Right now Relay is only really usable in \"simple proxy mode\" (without credentials), and as such calls the same exact endpoints on Sentry that an SDK would. That also means you have to configure each project individually in Relay. Create a new file in the form project_id.json : .relay / projects / ___PROJECT_ID___.json With the following content: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ], \"config\" : { \"allowedDomains\" : [ \"*\" ], \"piiConfig\" : {} } } The relay has to know all public keys (i.e. the secret part of the DSN) that will send events to it. DSNs unknown for this project will be rejected.","title":"Setting up a Project"},{"location":"#sending-a-test-event","text":"Launch the server with relay run , and set up any SDK with the following DSN: http : //___PUBLIC_KEY___@127.0.0.1:3000/___PROJECT_ID___ As you can see we only changed the host and port of the DSN to point to your Relay setup. You should be able to use the SDK normally at this point. Events arrive at the Sentry instance that Relay is configured to use in .relay/config.yml .","title":"Sending a Test Event"},{"location":"#pii-stripping","text":"Now let's get to the entire point of this proxy setup: Stripping sensitive data. The easiest way to go about this is if you already have a raw JSON payload from some SDK. Go to our PII config editor Piinguin , and: Paste in a raw event Click on data you want eliminated Paste in other payloads and see if they look fine, go to step 2 if necessary. After iterating on the config, paste it back into the project config you created earlier: .relay / projects / ___PROJECT_ID___.json For example: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ], \"config\" : { \"allowedDomains\" : [ \"*\" ], \"piiConfig\" : { \"rules\" : { \"device_id\" : { \"type\" : \"pattern\" , \"pattern\" : \"d/[a-f0-9]{12}\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"freeform\" : [ \"device_id\" ] } } } }","title":"PII Stripping"},{"location":"metrics/","text":"Metrics Relay emits statsd metrics in order to allow its monitoring. RelayHistograms Histogram metrics used by Relay. event.queue_size.pct The metric is referred in code as: RelayHistograms::EventQueueSizePct . The number of events in the queue as a percentage of the maximum number of events that can be stored in the queue ( 0 ... the queue is empty, 1 ... the queue is full and no additional events can be added). event.queue_size The metric is referred in code as: RelayHistograms::EventQueueSize . The number of events in the queue. The event queue represents the events that are being processed at a particular time in Relay. Once a request is received the event has some preliminary (quick) processing to determine if it can be processed or it is rejected. Once this determination has been done the http request that created the event terminates and, if the request is to be further processed, the event enters a queue ( a virtual queue, the event is kept in a future that will resolve at some point in time). Once the event finishes processing and is sent downstream (i.e. the future is resolved and the event leaves relay) the event is considered handled and it leaves the queue ( the queue size is decremented). event.size_bytes.raw The metric is referred in code as: RelayHistograms::EventSizeBytesRaw . The event size as seen by Relay after it is extracted from a request. event.size_bytes.uncompressed The metric is referred in code as: RelayHistograms::EventSizeBytesUncompressed . The event size as seen by Relay after it has been decompressed and decoded (e.g. from Base64). project_state.pending The metric is referred in code as: RelayHistograms::ProjectStatePending . Number of projects in the ProjectCache that are waiting for their state to be updated. project_state.request.batch_size The metric is referred in code as: RelayHistograms::ProjectStateRequestBatchSize . Number of project state requested from the Upstream for the current batch request. project_state.received The metric is referred in code as: RelayHistograms::ProjectStateReceived . Number of project states received from the Upstream for the current batch request. project_cache.size The metric is referred in code as: RelayHistograms::ProjectStateCacheSize . Number of project states currently held in the ProjectState cache. RelayTimers Timer metrics used by Relay event_processing.deserialize The metric is referred in code as: RelayTimers::EventProcessingDeserialize . The time spent deserializing an event from a JSON byte array into the native data structure on which Relay operates. event_processing.process Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayTimers::EventProcessingProcess . Time spent running event processors on an event. Event processing happens before filtering. event_processing.filtering Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayTimers::EventProcessingFiltering . Time spent running filtering on an event. event_processing.rate_limiting Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayTimers::EventProcessingRateLimiting . Time spent checking for rate limits in Redis. Note that not all events are checked against Redis. After an event is rate limited for period A, any event using the same key coming during period A will be automatically rate limited without checking against Redis (the event will be simply discarded without being placed in the processing queue). event_processing.pii The metric is referred in code as: RelayTimers::EventProcessingPii . Time spent in data scrubbing for the current event. event_processing.serialization The metric is referred in code as: RelayTimers::EventProcessingSerialization . Time spent converting the event from an Annotated into a String containing the JSON representation of the event. event.wait_time The metric is referred in code as: RelayTimers::EventWaitTime . Represents the time spent between receiving the event in Relay (i.e. beginning of the request handling) up to the time before starting synchronous processing in the EventProcessor. event.processing_time The metric is referred in code as: RelayTimers::EventProcessingTime . This is the time the event spends in the EventProcessor (i.e. the sync processing of the event). The time spent in synchronous event processing. This timing covers the end-to-end processing in the CPU pool and comprises: event_processing.deserialize event_processing.pii event_processing.serialization With Relay in processing mode, this includes the following additional timings: event_processing.process event_processing.filtering event_processing.rate_limiting event.total_time The metric is referred in code as: RelayTimers::EventTotalTime . The total time an event spends in Relay from the time it is received until it finishes processing. project_state.eviction.duration The metric is referred in code as: RelayTimers::ProjectStateEvictionDuration . The total time spent during ProjectCache.fetch_states in which eviction of outdated projects happens. project_state.request.duration The metric is referred in code as: RelayTimers::ProjectStateRequestDuration . The total time spent during ProjectCache.fetch_states spent waiting for all ProjectState requests to resolve. During a fetch_states request, we pick up to max_num_requests * max_num_project_states_per_request projects that need their state updated and batch them into max_num_requests requests. This metric represents the time spent from issuing the first request until all requests are finished. project_id.request.duration The metric is referred in code as: RelayTimers::ProjectIdRequestDuration . The total time spent getting the project id from upstream. Note that ProjectIdRequests happen only for the legacy endpoint that does not specify the project id in the url, for the new endpoints the project id is extracted from the url path. Only projects with the id not already fetched are counted. The project id is only fetched once and it is not refreshed. requests.duration The metric is referred in code as: RelayTimers::RequestsDuration . The total duration of a request as seen from Relay from the moment the request is received until a http result is returned. Note that this does not represent the total duration for processing an event. Requests for events that are not immediately rejected ( because the project has hit a rate limit) are scheduled for processing at a latter time and an HTTP OK (200) is returned. RelaySets Set metrics used by Relay unique_projects The metric is referred in code as: RelaySets::UniqueProjects . Represents the number of active projects in the current slice of time RelayCounters Counter metrics used by Relay event.accepted The metric is referred in code as: RelayCounters::EventAccepted . Number of events accepted in the current time slot. This represents events that have successfully passed rate limits, filters and have been successfully handled. event.rejected The metric is referred in code as: RelayCounters::EventRejected . Number of events rejected in the current time slot. This includes events being rejected because they are malformed or any other error during processing (including filtered events, discarded events and rate limited events). events.outcomes Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayCounters::EventOutcomes . Represents a group of counters, implemented with using tags. The following tags are present for each event outcome: outcome which is an EventOutcome enumeration reason which is the reason string for all outcomes that are not Accepted . project_state.get The metric is referred in code as: RelayCounters::ProjectStateGet . Counts the number of times a project state lookup is done. This includes requests for projects that are cached and requests for projects that are not yet cached. All requests that return a EventAction::Accept i.e. are not rate limited (on the fast path) or are discarded because we know the project is disabled or invalid will be counted. project_state.request The metric is referred in code as: RelayCounters::ProjectStateRequest . Counts the number of project state http requests. Note that a project state HTTP request typically contains a number of projects (the project state requests are batched). project_cache.hit The metric is referred in code as: RelayCounters::ProjectCacheHit . Counts the number of times a request for a project is already present, this effectively represents the fraction of project_state.get that will not result in a ProjectState request. project_cache.miss The metric is referred in code as: RelayCounters::ProjectCacheMiss . Counts the number of times a request for a project is not already present. project_state.get = project_cache.miss + project_cache.hit . Requests that are generating a cache hit will be queued and batched and eventually will generate a project_state.request . project_id.request The metric is referred in code as: RelayCounters::ProjectIdRequest . Counts the number of requests for the ProjectId (the timing is tracked by project_id.request.duration ). Note that ProjectIdRequests happen only for the legacy endpoint that does not specify the project id in the url, for the new endpoints the project id is extracted from the url path. Only projects with the id not already fetched are counted. Once the ProjectId is successfully cached it will be retained indefinitely. server.starting The metric is referred in code as: RelayCounters::ServerStarting . Counts the number of times Relay started. This can be used to track unwanted restarts due to crashes or termination. processing.event.produced Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayCounters::ProcessingEventProduced . Counts the number of messages placed on the Kafka queue. When Relay operates with processing enabled and a message is successfully processed each message will generate an event on the Kafka queue and zero or more attachments. The counter has an event_type tag which is set to either event or attachment representing the type of message produced on the Kafka queue. event.protocol The metric is referred in code as: RelayCounters::EventProtocol . Counts the number of events that hit any of the Store like endpoints (Store, Security, MiniDump, Unreal). The events are counted before they are rate limited , filtered or processed in any way. The counter has a version tag that tracks the message event protocol version. requests The metric is referred in code as: RelayCounters::Requests . Counts the number of requests reaching Relay. responses.status_codes The metric is referred in code as: RelayCounters::ResponsesStatusCodes . Counts the number of requests that have finished during the current interval. The counter has the following tags: status_code The HTTP status code number. method The HTTP method used in the request in uppercase. route Unique dashed identifier of the endpoint. project_cache.eviction The metric is referred in code as: RelayCounters::EvictingStaleProjectCaches . We are scanning our in-memory project cache for stale entries. This counter is incremented before doing the expensive operation.","title":"Metrics"},{"location":"metrics/#metrics","text":"Relay emits statsd metrics in order to allow its monitoring.","title":"Metrics"},{"location":"metrics/#relayhistograms","text":"Histogram metrics used by Relay.","title":"RelayHistograms"},{"location":"metrics/#eventqueue_sizepct","text":"The metric is referred in code as: RelayHistograms::EventQueueSizePct . The number of events in the queue as a percentage of the maximum number of events that can be stored in the queue ( 0 ... the queue is empty, 1 ... the queue is full and no additional events can be added).","title":"event.queue_size.pct"},{"location":"metrics/#eventqueue_size","text":"The metric is referred in code as: RelayHistograms::EventQueueSize . The number of events in the queue. The event queue represents the events that are being processed at a particular time in Relay. Once a request is received the event has some preliminary (quick) processing to determine if it can be processed or it is rejected. Once this determination has been done the http request that created the event terminates and, if the request is to be further processed, the event enters a queue ( a virtual queue, the event is kept in a future that will resolve at some point in time). Once the event finishes processing and is sent downstream (i.e. the future is resolved and the event leaves relay) the event is considered handled and it leaves the queue ( the queue size is decremented).","title":"event.queue_size"},{"location":"metrics/#eventsize_bytesraw","text":"The metric is referred in code as: RelayHistograms::EventSizeBytesRaw . The event size as seen by Relay after it is extracted from a request.","title":"event.size_bytes.raw"},{"location":"metrics/#eventsize_bytesuncompressed","text":"The metric is referred in code as: RelayHistograms::EventSizeBytesUncompressed . The event size as seen by Relay after it has been decompressed and decoded (e.g. from Base64).","title":"event.size_bytes.uncompressed"},{"location":"metrics/#project_statepending","text":"The metric is referred in code as: RelayHistograms::ProjectStatePending . Number of projects in the ProjectCache that are waiting for their state to be updated.","title":"project_state.pending"},{"location":"metrics/#project_staterequestbatch_size","text":"The metric is referred in code as: RelayHistograms::ProjectStateRequestBatchSize . Number of project state requested from the Upstream for the current batch request.","title":"project_state.request.batch_size"},{"location":"metrics/#project_statereceived","text":"The metric is referred in code as: RelayHistograms::ProjectStateReceived . Number of project states received from the Upstream for the current batch request.","title":"project_state.received"},{"location":"metrics/#project_cachesize","text":"The metric is referred in code as: RelayHistograms::ProjectStateCacheSize . Number of project states currently held in the ProjectState cache.","title":"project_cache.size"},{"location":"metrics/#relaytimers","text":"Timer metrics used by Relay","title":"RelayTimers"},{"location":"metrics/#event_processingdeserialize","text":"The metric is referred in code as: RelayTimers::EventProcessingDeserialize . The time spent deserializing an event from a JSON byte array into the native data structure on which Relay operates.","title":"event_processing.deserialize"},{"location":"metrics/#event_processingprocess","text":"Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayTimers::EventProcessingProcess . Time spent running event processors on an event. Event processing happens before filtering.","title":"event_processing.process"},{"location":"metrics/#event_processingfiltering","text":"Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayTimers::EventProcessingFiltering . Time spent running filtering on an event.","title":"event_processing.filtering"},{"location":"metrics/#event_processingrate_limiting","text":"Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayTimers::EventProcessingRateLimiting . Time spent checking for rate limits in Redis. Note that not all events are checked against Redis. After an event is rate limited for period A, any event using the same key coming during period A will be automatically rate limited without checking against Redis (the event will be simply discarded without being placed in the processing queue).","title":"event_processing.rate_limiting"},{"location":"metrics/#event_processingpii","text":"The metric is referred in code as: RelayTimers::EventProcessingPii . Time spent in data scrubbing for the current event.","title":"event_processing.pii"},{"location":"metrics/#event_processingserialization","text":"The metric is referred in code as: RelayTimers::EventProcessingSerialization . Time spent converting the event from an Annotated into a String containing the JSON representation of the event.","title":"event_processing.serialization"},{"location":"metrics/#eventwait_time","text":"The metric is referred in code as: RelayTimers::EventWaitTime . Represents the time spent between receiving the event in Relay (i.e. beginning of the request handling) up to the time before starting synchronous processing in the EventProcessor.","title":"event.wait_time"},{"location":"metrics/#eventprocessing_time","text":"The metric is referred in code as: RelayTimers::EventProcessingTime . This is the time the event spends in the EventProcessor (i.e. the sync processing of the event). The time spent in synchronous event processing. This timing covers the end-to-end processing in the CPU pool and comprises: event_processing.deserialize event_processing.pii event_processing.serialization With Relay in processing mode, this includes the following additional timings: event_processing.process event_processing.filtering event_processing.rate_limiting","title":"event.processing_time"},{"location":"metrics/#eventtotal_time","text":"The metric is referred in code as: RelayTimers::EventTotalTime . The total time an event spends in Relay from the time it is received until it finishes processing.","title":"event.total_time"},{"location":"metrics/#project_stateevictionduration","text":"The metric is referred in code as: RelayTimers::ProjectStateEvictionDuration . The total time spent during ProjectCache.fetch_states in which eviction of outdated projects happens.","title":"project_state.eviction.duration"},{"location":"metrics/#project_staterequestduration","text":"The metric is referred in code as: RelayTimers::ProjectStateRequestDuration . The total time spent during ProjectCache.fetch_states spent waiting for all ProjectState requests to resolve. During a fetch_states request, we pick up to max_num_requests * max_num_project_states_per_request projects that need their state updated and batch them into max_num_requests requests. This metric represents the time spent from issuing the first request until all requests are finished.","title":"project_state.request.duration"},{"location":"metrics/#project_idrequestduration","text":"The metric is referred in code as: RelayTimers::ProjectIdRequestDuration . The total time spent getting the project id from upstream. Note that ProjectIdRequests happen only for the legacy endpoint that does not specify the project id in the url, for the new endpoints the project id is extracted from the url path. Only projects with the id not already fetched are counted. The project id is only fetched once and it is not refreshed.","title":"project_id.request.duration"},{"location":"metrics/#requestsduration","text":"The metric is referred in code as: RelayTimers::RequestsDuration . The total duration of a request as seen from Relay from the moment the request is received until a http result is returned. Note that this does not represent the total duration for processing an event. Requests for events that are not immediately rejected ( because the project has hit a rate limit) are scheduled for processing at a latter time and an HTTP OK (200) is returned.","title":"requests.duration"},{"location":"metrics/#relaysets","text":"Set metrics used by Relay","title":"RelaySets"},{"location":"metrics/#unique_projects","text":"The metric is referred in code as: RelaySets::UniqueProjects . Represents the number of active projects in the current slice of time","title":"unique_projects"},{"location":"metrics/#relaycounters","text":"Counter metrics used by Relay","title":"RelayCounters"},{"location":"metrics/#eventaccepted","text":"The metric is referred in code as: RelayCounters::EventAccepted . Number of events accepted in the current time slot. This represents events that have successfully passed rate limits, filters and have been successfully handled.","title":"event.accepted"},{"location":"metrics/#eventrejected","text":"The metric is referred in code as: RelayCounters::EventRejected . Number of events rejected in the current time slot. This includes events being rejected because they are malformed or any other error during processing (including filtered events, discarded events and rate limited events).","title":"event.rejected"},{"location":"metrics/#eventsoutcomes","text":"Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayCounters::EventOutcomes . Represents a group of counters, implemented with using tags. The following tags are present for each event outcome: outcome which is an EventOutcome enumeration reason which is the reason string for all outcomes that are not Accepted .","title":"events.outcomes"},{"location":"metrics/#project_stateget","text":"The metric is referred in code as: RelayCounters::ProjectStateGet . Counts the number of times a project state lookup is done. This includes requests for projects that are cached and requests for projects that are not yet cached. All requests that return a EventAction::Accept i.e. are not rate limited (on the fast path) or are discarded because we know the project is disabled or invalid will be counted.","title":"project_state.get"},{"location":"metrics/#project_staterequest","text":"The metric is referred in code as: RelayCounters::ProjectStateRequest . Counts the number of project state http requests. Note that a project state HTTP request typically contains a number of projects (the project state requests are batched).","title":"project_state.request"},{"location":"metrics/#project_cachehit","text":"The metric is referred in code as: RelayCounters::ProjectCacheHit . Counts the number of times a request for a project is already present, this effectively represents the fraction of project_state.get that will not result in a ProjectState request.","title":"project_cache.hit"},{"location":"metrics/#project_cachemiss","text":"The metric is referred in code as: RelayCounters::ProjectCacheMiss . Counts the number of times a request for a project is not already present. project_state.get = project_cache.miss + project_cache.hit . Requests that are generating a cache hit will be queued and batched and eventually will generate a project_state.request .","title":"project_cache.miss"},{"location":"metrics/#project_idrequest","text":"The metric is referred in code as: RelayCounters::ProjectIdRequest . Counts the number of requests for the ProjectId (the timing is tracked by project_id.request.duration ). Note that ProjectIdRequests happen only for the legacy endpoint that does not specify the project id in the url, for the new endpoints the project id is extracted from the url path. Only projects with the id not already fetched are counted. Once the ProjectId is successfully cached it will be retained indefinitely.","title":"project_id.request"},{"location":"metrics/#serverstarting","text":"The metric is referred in code as: RelayCounters::ServerStarting . Counts the number of times Relay started. This can be used to track unwanted restarts due to crashes or termination.","title":"server.starting"},{"location":"metrics/#processingeventproduced","text":"Note : This metric is emitted only when Relay is built with the processing feature. The metric is referred in code as: RelayCounters::ProcessingEventProduced . Counts the number of messages placed on the Kafka queue. When Relay operates with processing enabled and a message is successfully processed each message will generate an event on the Kafka queue and zero or more attachments. The counter has an event_type tag which is set to either event or attachment representing the type of message produced on the Kafka queue.","title":"processing.event.produced"},{"location":"metrics/#eventprotocol","text":"The metric is referred in code as: RelayCounters::EventProtocol . Counts the number of events that hit any of the Store like endpoints (Store, Security, MiniDump, Unreal). The events are counted before they are rate limited , filtered or processed in any way. The counter has a version tag that tracks the message event protocol version.","title":"event.protocol"},{"location":"metrics/#requests","text":"The metric is referred in code as: RelayCounters::Requests . Counts the number of requests reaching Relay.","title":"requests"},{"location":"metrics/#responsesstatus_codes","text":"The metric is referred in code as: RelayCounters::ResponsesStatusCodes . Counts the number of requests that have finished during the current interval. The counter has the following tags: status_code The HTTP status code number. method The HTTP method used in the request in uppercase. route Unique dashed identifier of the endpoint.","title":"responses.status_codes"},{"location":"metrics/#project_cacheeviction","text":"The metric is referred in code as: RelayCounters::EvictingStaleProjectCaches . We are scanning our in-memory project cache for stale entries. This counter is incremented before doing the expensive operation.","title":"project_cache.eviction"},{"location":"options/","text":"Configuration Options The base configuration for Relay lives in the file .relay/config.yml . All keys are snake_case . Relay General relay settings. relay.mode : string, default: managed Controls the basic communication and configuration mode for this relay. Allowed values are: managed (default) : Project configurations are managed by Sentry, unless they are statically overridden via the file system. This requires credentials to be set up and white listed in Sentry. static : Projects must be statically configured on the file system. If configured, PII stripping is also performed on those events. Events for unknown projects are automatically rejected. proxy : Relay acts as a proxy for all requests and events. It will not load project configurations from the upstream or perform PII stripping. All events are accepted unless overridden on the file system. For more information on providing or overriding project configurations on the file system, please refer to Project Configuration and PII Configuration . relay.upstream : string, default: https://sentry.io The upstream relay or sentry instance. Important : Relay does not check for cycles. Ensure this option is not set to an endpoint that will cause events to be cycled back here. relay.host : string, default: 127.0.0.1 The host the relay should bind to (network interface). Example: 0.0.0.0 relay.port : integer, default: 3000 The port to bind for the unencrypted relay HTTP server. Example: 3000 relay.tls_port : integer, optional Optional port to bind for the encrypted relay HTTPS server. Example: 3001 This is in addition to the port option: If you set up a HTTPS server at tls_port , the HTTP server at port still exists. relay.tls_identity_path : string, optional The filesystem path to the identity (DER-encoded PKCS12) to use for the HTTPS server. Example: relay_dev.pfx relay.tls_identity_password : string, optional Password for the PKCS12 archive in tls_identity_path . HTTP Set various network-related settings. http.timeout : integer, default: 5 Timeout for upstream requests in seconds. http.max_retry_interval : integer, default: 60 Maximum interval between failed request retries in seconds. Caching Fine-tune caching of project state. cache.project_expiry : integer, default: 300 (5 minutes) The cache timeout for project configurations in seconds. Irrelevant if you use the \"simple proxy mode\", where your project config is stored in a local file. cache.relay_expiry : integer, default: 3600 (1 hour) The cache timeout for downstream relay info (public keys) in seconds. cache.event_expiry : integer, default: 600 (10 minutes) The cache timeout for events (store) before dropping them. cache.miss_expiry : integer, default: 60 (1 minute) The cache timeout for non-existing entries. cache.batch_interval : integer, default: 100 (100 milliseconds) The buffer timeout for batched queries before sending them upstream in milliseconds . cache.file_interval : integer, default: 10 (10 seconds) Interval for watching local cache override files in seconds. cache.event_buffer_size : integer, default: 1000 The maximum number of events that are buffered in case of network issues or high rates of incoming events. Size Limits Controls various HTTP-related limits. All values are human-readable strings of a number and a human-readable unit, such as: 1KiB 1MB 1MiB 1MiB 1025B limits.max_concurrent_requests : integer, default: 100 The maximum number of concurrent connections to the upstream. limits.max_event_payload_size : string, default: 256KB The maximum payload size for events. limits.max_api_payload_size : string, default: 20MB The maximum payload size for general API requests. limits.max_api_file_upload_size : string, default: 40MB The maximum payload size for file uploads and chunks. limits.max_api_chunk_upload_size : string, default: 100MB The maximum payload size for chunks Logging logging.level : string, default: info The log level for the relay. One of: off error warn info debug trace logging.log_failed_payloads : boolean, default: false If set to true this emits log messages for failed event payloads. logging.format : string, default: auto Controls the log format. One of: auto : Auto detect (pretty for tty, simplified for other) pretty : With colors simplified : Simplified log output json : Dump out JSON lines logging.enable_backtraces : boolean, default: true When set to true, backtraces are forced on. Statsd Metrics metrics.statsd : string, optional If set to a host/port string then metrics will be reported to this statsd instance. metrics.prefix : string, default: sentry.relay The prefix that should be added to all metrics. Internal Error Reporting Configures error reporting for errors happening within Sentry. Disabled by default. sentry.enabled : boolean, default: false Whether to report internal errors to a separate DSN. false means no internal errors are sent (just logged). sentry.dsn : string, optional DSN to report internal relay failures to. It is not a good idea to set this to a value that will make the relay send errors to itself. Ideally this should just send errors to Sentry directly, not another relay.","title":"Configuration Options"},{"location":"options/#configuration-options","text":"The base configuration for Relay lives in the file .relay/config.yml . All keys are snake_case .","title":"Configuration Options"},{"location":"options/#relay","text":"General relay settings. relay.mode : string, default: managed Controls the basic communication and configuration mode for this relay. Allowed values are: managed (default) : Project configurations are managed by Sentry, unless they are statically overridden via the file system. This requires credentials to be set up and white listed in Sentry. static : Projects must be statically configured on the file system. If configured, PII stripping is also performed on those events. Events for unknown projects are automatically rejected. proxy : Relay acts as a proxy for all requests and events. It will not load project configurations from the upstream or perform PII stripping. All events are accepted unless overridden on the file system. For more information on providing or overriding project configurations on the file system, please refer to Project Configuration and PII Configuration . relay.upstream : string, default: https://sentry.io The upstream relay or sentry instance. Important : Relay does not check for cycles. Ensure this option is not set to an endpoint that will cause events to be cycled back here. relay.host : string, default: 127.0.0.1 The host the relay should bind to (network interface). Example: 0.0.0.0 relay.port : integer, default: 3000 The port to bind for the unencrypted relay HTTP server. Example: 3000 relay.tls_port : integer, optional Optional port to bind for the encrypted relay HTTPS server. Example: 3001 This is in addition to the port option: If you set up a HTTPS server at tls_port , the HTTP server at port still exists. relay.tls_identity_path : string, optional The filesystem path to the identity (DER-encoded PKCS12) to use for the HTTPS server. Example: relay_dev.pfx relay.tls_identity_password : string, optional Password for the PKCS12 archive in tls_identity_path .","title":"Relay"},{"location":"options/#http","text":"Set various network-related settings. http.timeout : integer, default: 5 Timeout for upstream requests in seconds. http.max_retry_interval : integer, default: 60 Maximum interval between failed request retries in seconds.","title":"HTTP"},{"location":"options/#caching","text":"Fine-tune caching of project state. cache.project_expiry : integer, default: 300 (5 minutes) The cache timeout for project configurations in seconds. Irrelevant if you use the \"simple proxy mode\", where your project config is stored in a local file. cache.relay_expiry : integer, default: 3600 (1 hour) The cache timeout for downstream relay info (public keys) in seconds. cache.event_expiry : integer, default: 600 (10 minutes) The cache timeout for events (store) before dropping them. cache.miss_expiry : integer, default: 60 (1 minute) The cache timeout for non-existing entries. cache.batch_interval : integer, default: 100 (100 milliseconds) The buffer timeout for batched queries before sending them upstream in milliseconds . cache.file_interval : integer, default: 10 (10 seconds) Interval for watching local cache override files in seconds. cache.event_buffer_size : integer, default: 1000 The maximum number of events that are buffered in case of network issues or high rates of incoming events.","title":"Caching"},{"location":"options/#size-limits","text":"Controls various HTTP-related limits. All values are human-readable strings of a number and a human-readable unit, such as: 1KiB 1MB 1MiB 1MiB 1025B limits.max_concurrent_requests : integer, default: 100 The maximum number of concurrent connections to the upstream. limits.max_event_payload_size : string, default: 256KB The maximum payload size for events. limits.max_api_payload_size : string, default: 20MB The maximum payload size for general API requests. limits.max_api_file_upload_size : string, default: 40MB The maximum payload size for file uploads and chunks. limits.max_api_chunk_upload_size : string, default: 100MB The maximum payload size for chunks","title":"Size Limits"},{"location":"options/#logging","text":"logging.level : string, default: info The log level for the relay. One of: off error warn info debug trace logging.log_failed_payloads : boolean, default: false If set to true this emits log messages for failed event payloads. logging.format : string, default: auto Controls the log format. One of: auto : Auto detect (pretty for tty, simplified for other) pretty : With colors simplified : Simplified log output json : Dump out JSON lines logging.enable_backtraces : boolean, default: true When set to true, backtraces are forced on.","title":"Logging"},{"location":"options/#statsd-metrics","text":"metrics.statsd : string, optional If set to a host/port string then metrics will be reported to this statsd instance. metrics.prefix : string, default: sentry.relay The prefix that should be added to all metrics.","title":"Statsd Metrics"},{"location":"options/#internal-error-reporting","text":"Configures error reporting for errors happening within Sentry. Disabled by default. sentry.enabled : boolean, default: false Whether to report internal errors to a separate DSN. false means no internal errors are sent (just logged). sentry.dsn : string, optional DSN to report internal relay failures to. It is not a good idea to set this to a value that will make the relay send errors to itself. Ideally this should just send errors to Sentry directly, not another relay.","title":"Internal Error Reporting"},{"location":"project-config/","text":"Project Configuration See Getting Started for a short introduction to project configs. The configuration for a project with the ID 123 lives in the file .relay/projects/123.json . You can get the project ID from the path section of a DSN. This page enumerates a few options that may be relevant for running Relay in proxy mode. All other options are currently not stable enough that we feel confident in documenting them. Basic Options disabled { \"disabled\" : false } Whether the project is disabled. If set to true , the Relay will drop all events sent to this project. publicKeys { \"publicKeys\" : [{ \"publicKey\" : \"deadbeef\" , \"isEnabled\" : true }]} A map enumerating known public keys (the public key in a DSN) and whether events using that key should be accepted. config.allowedDomains { \"config\" : { \"allowedDomains\" : [ \"*\" ]}} Configure origin URLs which Sentry should accept events from. This is corresponds to the \"Allowed Domains\" setting in the Sentry UI. Note that an empty array will reject all origins. Use the default [\"*\"] to allow all origins. config.piiConfig See PII Configuration .","title":"Project Configuration"},{"location":"project-config/#project-configuration","text":"See Getting Started for a short introduction to project configs. The configuration for a project with the ID 123 lives in the file .relay/projects/123.json . You can get the project ID from the path section of a DSN. This page enumerates a few options that may be relevant for running Relay in proxy mode. All other options are currently not stable enough that we feel confident in documenting them.","title":"Project Configuration"},{"location":"project-config/#basic-options","text":"","title":"Basic Options"},{"location":"project-config/#disabled","text":"{ \"disabled\" : false } Whether the project is disabled. If set to true , the Relay will drop all events sent to this project.","title":"disabled"},{"location":"project-config/#publickeys","text":"{ \"publicKeys\" : [{ \"publicKey\" : \"deadbeef\" , \"isEnabled\" : true }]} A map enumerating known public keys (the public key in a DSN) and whether events using that key should be accepted.","title":"publicKeys"},{"location":"project-config/#configalloweddomains","text":"{ \"config\" : { \"allowedDomains\" : [ \"*\" ]}} Configure origin URLs which Sentry should accept events from. This is corresponds to the \"Allowed Domains\" setting in the Sentry UI. Note that an empty array will reject all origins. Use the default [\"*\"] to allow all origins.","title":"config.allowedDomains"},{"location":"project-config/#configpiiconfig","text":"See PII Configuration .","title":"config.piiConfig"},{"location":"architecture/","text":"Architecture This subsection contains internal docs that are useful for development and operation of Relay. Important Note : Relay is undergoing extensive internal restructuring. This includes: The introduction of Envelopes for event ingestion. Updated rate limits to handle items different from each other. Changes to the project configuration and fetching mechanism. Optimizations in the rate limiting fast path. Once these changes are completed, this document will be updated with new descriptions about event ingestion and the architecture.","title":"Architecture"},{"location":"architecture/#architecture","text":"This subsection contains internal docs that are useful for development and operation of Relay. Important Note : Relay is undergoing extensive internal restructuring. This includes: The introduction of Envelopes for event ingestion. Updated rate limits to handle items different from each other. Changes to the project configuration and fetching mechanism. Optimizations in the rate limiting fast path. Once these changes are completed, this document will be updated with new descriptions about event ingestion and the architecture.","title":"Architecture"},{"location":"architecture/actors/","text":"Actors This document describes how Relay works through the perspective of the system actors and the messages exchanged by them. TODO Short description about infrastructure (i.e. actix-web, actix, tokio, futures), note that we are using the old style Future trait, and actix 0.7.x . controller.rs events.rs The events.rs module contains functionality related to the processing of events. Raw events being sent to the system are first processed, and then sent to Sentry for saving. The module contains to actors: EventManager : an actor that receives all events, checks if the event should be processed (i.e. it is not filtered) and orchestrates all data needed for event processing. If the event should be processed it is passed to the EventProcessor . There is one EventManager in the system. EventProcessor : an CPU bound actor that receives all necessary project configuration and does the heavy lifting of processing the message (normalisation, PII stripping ...). There are multiple EventProcessor actors running in a thread pool. EventManager The EventManager is an actor running in the main system arbiter. The system arbiter is a asynchronous arbiter created when Relay starts. The upshot of this is that all processing done by the event manager should be extremly quick. It is ok to wait on I/O but no significant processing may be done in the EventManager . Once the EventManager had obtained the project state and had decided that the event should be processed the EventManager passes the event to the EventProcessor actors. TODO document all handlers EventProcessor The EventProcessor is an actor running in a SyncArbiter. There are multiple instances of the EventProcessor actor (roughly 1 per thread) ( TODO RaduW check that I'm not talking nonsense here.). The EventProcessor does the heavy lifting of event processing. The event processing does only synchronous work ( all the IO is handleed in the EventManager and all the needed state is passed to the EventProcessor ) Since all the work done by the event processor is the synchronous processing of an event there is only one type of message accepted by the EventProcessor actor: ProcessEvent The ProcessEvent handler prepares the event for ingestion. It normalizes the event, symbolicates its stack-trace and strips sensitive information (PPI stripping). TODO finish here keys.rs outcome.rs server.rs store.rs upstream.rs project.rs The project.rs module contains functionality related to the project state. Sentry events belong to projects and projects belong to organizations (Relay doesn't care at the moment about organizations). Projects serve to group the events (so that each Sentry customer can only see and deal with his/hers own events) and prescribe how messages are to be processed ( what should be filtered, which data inside the event should be anonymised (i.e PPI stripping), etc). All activities around obtaining, caching and refreshing the project state is handled in this module. The module contains two actors: Project : an actor that holds project data ProjectCache : an actor that holds references to Project actors. From a high level perspective one obtains a Project from the ProjectCache and then uses the Project in order to get 'project specific' information. Project The Project actor is responsible with decisions about how an event for a particular project should be handled. The project runs in an async arbiter ( I think it actully runs in the SystemArbiter TODO check ir that is true, or if it runs in another async arbiter). The system constructs an actor for each project used. The Project actor runs in an async context. The Project actor responds to the following message types: GetProjectId The GetProjectId handler returns the project_id (trivial functionality). GetProjectState The handler retuns the ProjectState , either directly (if it has a recent cached version) or by asking the ProjectCache to fetch it on its behalf. Pseudo Code if 'we have an up-to-date project state' return self . project_state if not self . receiver : # we don't have an already active request for the project state channel , receiver = 'create a channel and save its receiver' async : # ops that run at some time in the future project_state = await ProjectCache . send ( FetchProjectState ) channel . sender . push ( project_state ) # put the state on the channel when available channel = None # clenaup after we have pushed the ProjectState return future ( receiver ) # a Future that resolves with the ProjectState when available GetEventAction The handler answers the question: How should an event be handled? An event may be handled in one of three ways specified by the EventAction enum. pub enum EventAction { /// The event should be discarded. Discard ( DiscardReason ), /// The event should be discarded and the client should back off for some time. RetryAfter ( RateLimit ), /// The event should be processed and sent to upstream. Accept , } Pseudo Code if 'we have a cached rate limit value for the current event key' : return RetryAfter # the event is discarded without further processing if 'we have a recent project state' # Ask the ProjectState what to do with the msg return projectState . get_event_action ( event ) # No recent ProjectState, fetch it from the ProjectCache actor async : # ops that run at soem time in the future projectState = await ProjectCache . send ( FetchProjectState ) # return a future that will resolve when the project state becomes avaialble return future ( projectState . get_event_action ( event )) RateLimit The handler is used to update chached versions of project RateLimits. Pseudo Code if rate_limit . type not in self . rate_limit_cache self . rate_limit_cache . insert ( rate_limit ) # insert a new rate limit else if \"rate_limit expires sooner than the old cached value\" : self . rate_limit_cache . insert ( rate_limit ) # update old rate limit ProjectCache The ProjectCache actor is responsible for providing Project actors. The ProjectCache runs in an asynchronous context. There is only one instance of the ProjectCache in the system. The project runs in an async arbiter ( I think it actully runs in the SystemArbiter TODO check ir that is true, or if it runs in another async arbiter). The ProjectCache actor responds to the following messages: GetProject Retuns a Project actor for the required project. Pseudo Code if project_id not in self . project_cache : project = Project ( project_id ) project . start () self . project_cache [ project_id ] = project return self . project_cache [ porject_id ] FetchProjectState Registers the intention of a project to retrieve the project state. The FetchProjectState functionality logicaly belongs to the Project actor but it is handled by the ProjectCache in order to batch requests for the project state from multiple Project actors. A Project registers its desire to obtain its project state with the ProjectCache by sending the FetchProjectState and the ProjectCache batches all requests during a batching period and emits one request to the upstream (another Relay or the Sentry server) for all required project states. The handler checks if there is already a scheduled time for fetching project states and, if not, it schedules a delayed function that will do the actual fetching for all projects that registered their desire for getting the project state (see function: ProjectCache::fetch_states ). TODO: Add pseudo code UpdateLocalStates Updates project states for project states comming from a local file configuration.(TODO check if that is correct RaduW. 10.Oct.2019) This message is emmited periodically by a thread that loops and periodically checks the local file system for configuration changes (see function: poll_local_states ). Store endpoint request processing graph LR Store>/api/#PRJ#/store<br/>store.storeEvent] Upstream(UpstreamRelay) EvMan(EventManager) Proj(Project) EvProc[\"EventProcessor <br/>(sync)\"] Cache(ProjectCache) StoreF(StoreForwarder) Store-->|GetEventAction|Proj Store-->|GetProject|Cache Store-->|QueueEvent|EvMan Proj-->|FetchProjectState|Cache EvMan-->|GetProjectState|Proj EvMan-->|GetEventAction|Proj EvMan-->|RateLimit|Proj EvMan-->|HandleEvent|EvMan EvMan-->|GetProjectId|Proj EvMan-->|ProcessEvent|EvProc EvMan-->|StoreEvent|StoreF EvMan-->|SendRequest|Upstream Cache-->|UpdateLocalStates|Cache Event ingestion Title : Event Ingestion store_event --> ProjectCache : GetProject ProjectCache --> store_event : Project store_event --> Project : GetEventAction Project --> store_event : EventAction store_event --> EventManager : QueueEvent EventManager --> store_event : event_id EventManager --> EventManager : HandleEvent EventManager --> Project : GetProjectId Project --> EventManager : ProjectId EventManager --> Project : GetEventAction Project --> EventManager : EventAction EventManager --> Project : GetProjectState Project --> EventManager : ProjectState EventManager --> EventProcessor : ProcessEvent EventProcessor --> EventManager : ProcessEventResponse Server bootstrap main.main->cli.execute: cli.execute->cli.execute:make_app cli.execute->server.lib.run: server.lib.run->Controller: run Controller->server.actors.Server: start server.actors.Server->server.service:start server.service->ServiceState: start ServiceState start ServiceState->UpstreamRelay:start(config) ServiceState->EventManager:start(config, upstream_realy) ServiceState->KeyCache:start(config,upstream_relay) ServiceState->ProjectCache:start(config, upstream_relay)","title":"Actors"},{"location":"architecture/actors/#actors","text":"This document describes how Relay works through the perspective of the system actors and the messages exchanged by them. TODO Short description about infrastructure (i.e. actix-web, actix, tokio, futures), note that we are using the old style Future trait, and actix 0.7.x .","title":"Actors"},{"location":"architecture/actors/#controllerrs","text":"","title":"controller.rs"},{"location":"architecture/actors/#eventsrs","text":"The events.rs module contains functionality related to the processing of events. Raw events being sent to the system are first processed, and then sent to Sentry for saving. The module contains to actors: EventManager : an actor that receives all events, checks if the event should be processed (i.e. it is not filtered) and orchestrates all data needed for event processing. If the event should be processed it is passed to the EventProcessor . There is one EventManager in the system. EventProcessor : an CPU bound actor that receives all necessary project configuration and does the heavy lifting of processing the message (normalisation, PII stripping ...). There are multiple EventProcessor actors running in a thread pool.","title":"events.rs"},{"location":"architecture/actors/#eventmanager","text":"The EventManager is an actor running in the main system arbiter. The system arbiter is a asynchronous arbiter created when Relay starts. The upshot of this is that all processing done by the event manager should be extremly quick. It is ok to wait on I/O but no significant processing may be done in the EventManager . Once the EventManager had obtained the project state and had decided that the event should be processed the EventManager passes the event to the EventProcessor actors. TODO document all handlers","title":"EventManager"},{"location":"architecture/actors/#eventprocessor","text":"The EventProcessor is an actor running in a SyncArbiter. There are multiple instances of the EventProcessor actor (roughly 1 per thread) ( TODO RaduW check that I'm not talking nonsense here.). The EventProcessor does the heavy lifting of event processing. The event processing does only synchronous work ( all the IO is handleed in the EventManager and all the needed state is passed to the EventProcessor ) Since all the work done by the event processor is the synchronous processing of an event there is only one type of message accepted by the EventProcessor actor:","title":"EventProcessor"},{"location":"architecture/actors/#processevent","text":"The ProcessEvent handler prepares the event for ingestion. It normalizes the event, symbolicates its stack-trace and strips sensitive information (PPI stripping). TODO finish here","title":"ProcessEvent"},{"location":"architecture/actors/#keysrs","text":"","title":"keys.rs"},{"location":"architecture/actors/#outcomers","text":"","title":"outcome.rs"},{"location":"architecture/actors/#serverrs","text":"","title":"server.rs"},{"location":"architecture/actors/#storers","text":"","title":"store.rs"},{"location":"architecture/actors/#upstreamrs","text":"","title":"upstream.rs"},{"location":"architecture/actors/#projectrs","text":"The project.rs module contains functionality related to the project state. Sentry events belong to projects and projects belong to organizations (Relay doesn't care at the moment about organizations). Projects serve to group the events (so that each Sentry customer can only see and deal with his/hers own events) and prescribe how messages are to be processed ( what should be filtered, which data inside the event should be anonymised (i.e PPI stripping), etc). All activities around obtaining, caching and refreshing the project state is handled in this module. The module contains two actors: Project : an actor that holds project data ProjectCache : an actor that holds references to Project actors. From a high level perspective one obtains a Project from the ProjectCache and then uses the Project in order to get 'project specific' information.","title":"project.rs"},{"location":"architecture/actors/#project","text":"The Project actor is responsible with decisions about how an event for a particular project should be handled. The project runs in an async arbiter ( I think it actully runs in the SystemArbiter TODO check ir that is true, or if it runs in another async arbiter). The system constructs an actor for each project used. The Project actor runs in an async context. The Project actor responds to the following message types:","title":"Project"},{"location":"architecture/actors/#getprojectid","text":"The GetProjectId handler returns the project_id (trivial functionality).","title":"GetProjectId"},{"location":"architecture/actors/#getprojectstate","text":"The handler retuns the ProjectState , either directly (if it has a recent cached version) or by asking the ProjectCache to fetch it on its behalf.","title":"GetProjectState"},{"location":"architecture/actors/#pseudo-code","text":"if 'we have an up-to-date project state' return self . project_state if not self . receiver : # we don't have an already active request for the project state channel , receiver = 'create a channel and save its receiver' async : # ops that run at some time in the future project_state = await ProjectCache . send ( FetchProjectState ) channel . sender . push ( project_state ) # put the state on the channel when available channel = None # clenaup after we have pushed the ProjectState return future ( receiver ) # a Future that resolves with the ProjectState when available","title":"Pseudo Code"},{"location":"architecture/actors/#geteventaction","text":"The handler answers the question: How should an event be handled? An event may be handled in one of three ways specified by the EventAction enum. pub enum EventAction { /// The event should be discarded. Discard ( DiscardReason ), /// The event should be discarded and the client should back off for some time. RetryAfter ( RateLimit ), /// The event should be processed and sent to upstream. Accept , }","title":"GetEventAction"},{"location":"architecture/actors/#pseudo-code_1","text":"if 'we have a cached rate limit value for the current event key' : return RetryAfter # the event is discarded without further processing if 'we have a recent project state' # Ask the ProjectState what to do with the msg return projectState . get_event_action ( event ) # No recent ProjectState, fetch it from the ProjectCache actor async : # ops that run at soem time in the future projectState = await ProjectCache . send ( FetchProjectState ) # return a future that will resolve when the project state becomes avaialble return future ( projectState . get_event_action ( event ))","title":"Pseudo Code"},{"location":"architecture/actors/#ratelimit","text":"The handler is used to update chached versions of project RateLimits.","title":"RateLimit"},{"location":"architecture/actors/#pseudo-code_2","text":"if rate_limit . type not in self . rate_limit_cache self . rate_limit_cache . insert ( rate_limit ) # insert a new rate limit else if \"rate_limit expires sooner than the old cached value\" : self . rate_limit_cache . insert ( rate_limit ) # update old rate limit","title":"Pseudo Code"},{"location":"architecture/actors/#projectcache","text":"The ProjectCache actor is responsible for providing Project actors. The ProjectCache runs in an asynchronous context. There is only one instance of the ProjectCache in the system. The project runs in an async arbiter ( I think it actully runs in the SystemArbiter TODO check ir that is true, or if it runs in another async arbiter). The ProjectCache actor responds to the following messages:","title":"ProjectCache"},{"location":"architecture/actors/#getproject","text":"Retuns a Project actor for the required project.","title":"GetProject"},{"location":"architecture/actors/#pseudo-code_3","text":"if project_id not in self . project_cache : project = Project ( project_id ) project . start () self . project_cache [ project_id ] = project return self . project_cache [ porject_id ]","title":"Pseudo Code"},{"location":"architecture/actors/#fetchprojectstate","text":"Registers the intention of a project to retrieve the project state. The FetchProjectState functionality logicaly belongs to the Project actor but it is handled by the ProjectCache in order to batch requests for the project state from multiple Project actors. A Project registers its desire to obtain its project state with the ProjectCache by sending the FetchProjectState and the ProjectCache batches all requests during a batching period and emits one request to the upstream (another Relay or the Sentry server) for all required project states. The handler checks if there is already a scheduled time for fetching project states and, if not, it schedules a delayed function that will do the actual fetching for all projects that registered their desire for getting the project state (see function: ProjectCache::fetch_states ). TODO: Add pseudo code","title":"FetchProjectState"},{"location":"architecture/actors/#updatelocalstates","text":"Updates project states for project states comming from a local file configuration.(TODO check if that is correct RaduW. 10.Oct.2019) This message is emmited periodically by a thread that loops and periodically checks the local file system for configuration changes (see function: poll_local_states ). Store endpoint request processing graph LR Store>/api/#PRJ#/store<br/>store.storeEvent] Upstream(UpstreamRelay) EvMan(EventManager) Proj(Project) EvProc[\"EventProcessor <br/>(sync)\"] Cache(ProjectCache) StoreF(StoreForwarder) Store-->|GetEventAction|Proj Store-->|GetProject|Cache Store-->|QueueEvent|EvMan Proj-->|FetchProjectState|Cache EvMan-->|GetProjectState|Proj EvMan-->|GetEventAction|Proj EvMan-->|RateLimit|Proj EvMan-->|HandleEvent|EvMan EvMan-->|GetProjectId|Proj EvMan-->|ProcessEvent|EvProc EvMan-->|StoreEvent|StoreF EvMan-->|SendRequest|Upstream Cache-->|UpdateLocalStates|Cache Event ingestion Title : Event Ingestion store_event --> ProjectCache : GetProject ProjectCache --> store_event : Project store_event --> Project : GetEventAction Project --> store_event : EventAction store_event --> EventManager : QueueEvent EventManager --> store_event : event_id EventManager --> EventManager : HandleEvent EventManager --> Project : GetProjectId Project --> EventManager : ProjectId EventManager --> Project : GetEventAction Project --> EventManager : EventAction EventManager --> Project : GetProjectState Project --> EventManager : ProjectState EventManager --> EventProcessor : ProcessEvent EventProcessor --> EventManager : ProcessEventResponse Server bootstrap main.main->cli.execute: cli.execute->cli.execute:make_app cli.execute->server.lib.run: server.lib.run->Controller: run Controller->server.actors.Server: start server.actors.Server->server.service:start server.service->ServiceState: start ServiceState start ServiceState->UpstreamRelay:start(config) ServiceState->EventManager:start(config, upstream_realy) ServiceState->KeyCache:start(config,upstream_relay) ServiceState->ProjectCache:start(config, upstream_relay)","title":"UpdateLocalStates"},{"location":"architecture/ingest-event-path/","text":"Path of an Event through Relay Overview Simplified overview of event ingestion (ignores snuba/postprocessing): graph LR loadbalancer(Load Balancer) relay(Relay) projectconfigs(\"Project config endpoint (in Sentry)\") ingestconsumer(Ingest Consumer) outcomesconsumer(Outcomes Consumer) preprocess{\"<code>preprocess_event</code><br>(just a function call now)\"} process(<code>process_event</code>) save(<code>save_event</code>) loadbalancer-->relay relay---projectconfigs relay-->ingestconsumer relay-->outcomesconsumer ingestconsumer-->preprocess preprocess-->process preprocess-->save process-->save Processing enabled vs not? Relay can run as part of a Sentry installation, such as within sentry.io 's infrastructure, or next to the application as a forwarding proxy. A lot of steps described here are skipped or run in a limited form when Relay is not running with processing enabled: Event normalization does different (less) things. In certain modes, project config is not fetched from Sentry at all (but rather from disk or filled out with defaults). Events are forwarded to an HTTP endpoint instead of being written to Kafka. Rate limits are not calculated using Redis, instead Relay just honors 429s from previously mentioned endpoint. Filters are not applied at all. Inside the endpoint When an SDK hits /api/X/store on Relay, the code in server/src/endpoints/store.rs is called before returning a HTTP response. That code looks into an in-memory cache to answer basic information about a project such as: Does it exist? Is it suspended/disabled? Is it rate limited right now? If so, which key is rate limited? Which DSNs are valid for this project? Some of the data for this cache is coming from the projectconfigs endpoint . It is refreshed every couple of minutes, depending on configuration ( project_expiry ). If the cache is fresh, we may return a 429 for rate limits or a 4xx for invalid auth information. That cache might be empty or stale. If that is the case, Relay does not actually attempt to populate it at this stage. It just returns a 200 even though the event might be dropped later. This implies: The first store request that runs into a rate limit doesn't actually result in a 429 , but a subsequent request will (because by that time the project cache will have been updated). A store request for a non-existent project may result in a 200 , but subsequent ones will not. A store request with wrong auth information may result in a 200 , but subsequent ones will not. Filters are also not applied at this stage, so a filtered event will always result in a 200 . This matches the Python behavior since a while now . These examples assume that a project receives one event at a time. In practice one may observe that a highly concurrent burst of store requests for a single project results in 200 OK s only. However, a multi-second flood of incoming events should quickly result in eventually consistent and correct status codes. The response is completed at this point. All expensive work (such as talking to external services) is deferred into a background task. Except for responding to the HTTP request, there's no I/O done in the endpoint in any form. We didn't even hit Redis to calculate rate limits. Summary The HTTP response returned is just a best-effort guess at what the actual outcome of the event is going to be. We only return a 4xx code if we know that the response will fail (based on cached information), if we don't we return a 200 and continue to process the event asynchronously. This asynchronous processing used to happen synchronously in the Python implementation of StoreView . The effect of this is that the server will respond much faster that before but we might return 200 for events that will ultimately not be accepted. Generally Relay will return a 200 in many more situations than the old StoreView . The background task The HTTP response is out by now. The rest of what used to happen synchronously in the Python StoreView is done asynchronously, but still in the same process. So, now to the real work: Project config is fetched. If the project cache is stale or missing, we fetch it. We may wait a couple milliseconds ( batch_interval ) here to be able to batch multiple project config fetches into the same HTTP request to not overload Sentry too much. At this stage Relay may drop the event because it realized that the DSN was invalid or the project didn't even exist. The next incoming event will get a proper 4xx status code. The event is parsed. In the endpoint we only did decompression, a basic JSON syntax check, and extraction of the event ID to be able to return it as part of the response. Now we create an Event struct, which conceptually is the equivalent to parsing it into a Python dictionary: We allocate more memory. The event is normalized. Event normalization is probably the most CPU-intensive task running in Relay. It discards invalid data, moves data from deprecated fields to newer fields and generally just does schema validation. Filters (\"inbound filters\") are applied. Event may be discarded because of IP addresses, patterns on the error message or known web crawlers. Exact rate limits (\"quotas\") are applied. is_rate_limited.lua is executed on Redis. The input parameters for is_rate_limited.lua (\"quota objects\") are part of the project config. See this pull request for an explanation of what quota objects are. The event may be discarded here. If so, we write the rate limit info (reason and expiration timestamp) into the in-memory project cache so that the next store request returns a 429 in the endpoint and doesn't hit Redis at all. This contraption has the advantage that suspended or permanently rate-limited projects are very cheap to handle, and do not involve external services (ignoring the polling of the project config every couple of minutes). The event is datascrubbed. We have a PII config (new format) and a datascrubbing config (old format, converted to new format on the fly) as part of the project config fetched from Sentry. Event is written to Kafka. Note: If we discard an event at any point, an outcome is written to Kafka if Relay is configured to do so. Summary For events that returned a 200 we spawn an in-process background task that does the rest of what the old StoreView did. This task updates in-memory state for rate limits and disabled projects/keys. The outcomes consumer Outcomes are small messages in Kafka that contain an event ID and information about whether that event was rejected, and if so, why. The outcomes consumer is mostly responsible for updating (user-visible) counters in Sentry (buffers/counters and tsdb, which are two separate systems). The ingest consumer The ingest consumer reads accepted events from Kafka, and also updates some stats. Some of those stats are billing-relevant. Its main purpose is to do what insert_data_to_database in Python store did: Call preprocess_event , after which comes sourcemap processing, native symbolication, grouping, snuba and all that other stuff that is of no concern to Relay. Sequence diagram of components inside Relay sequenceDiagram participant sdk as SDK participant endpoint as Endpoint participant projectcache as ProjectCache participant eventmanager as EventManager participant cpupool as CPU Pool sdk->>endpoint:POST /api/42/store activate endpoint endpoint->>projectcache: get project (cached only) activate projectcache projectcache-->>endpoint: return project deactivate projectcache Note over endpoint: Checking rate limits and auth (fast path) endpoint->>eventmanager: queue event activate eventmanager eventmanager-->>endpoint:event ID endpoint-->>sdk:200 OK deactivate endpoint eventmanager->>projectcache:fetch project activate projectcache Note over eventmanager,projectcache: web request (batched with other projects) projectcache-->>eventmanager: return project deactivate projectcache eventmanager->>cpupool: . activate cpupool Note over eventmanager,cpupool: normalization, datascrubbing, redis rate limits, ... cpupool-->>eventmanager: . deactivate cpupool Note over eventmanager: Send event to kafka deactivate eventmanager","title":"Path of an Event through Relay"},{"location":"architecture/ingest-event-path/#path-of-an-event-through-relay","text":"","title":"Path of an Event through Relay"},{"location":"architecture/ingest-event-path/#overview","text":"Simplified overview of event ingestion (ignores snuba/postprocessing): graph LR loadbalancer(Load Balancer) relay(Relay) projectconfigs(\"Project config endpoint (in Sentry)\") ingestconsumer(Ingest Consumer) outcomesconsumer(Outcomes Consumer) preprocess{\"<code>preprocess_event</code><br>(just a function call now)\"} process(<code>process_event</code>) save(<code>save_event</code>) loadbalancer-->relay relay---projectconfigs relay-->ingestconsumer relay-->outcomesconsumer ingestconsumer-->preprocess preprocess-->process preprocess-->save process-->save","title":"Overview"},{"location":"architecture/ingest-event-path/#processing-enabled-vs-not","text":"Relay can run as part of a Sentry installation, such as within sentry.io 's infrastructure, or next to the application as a forwarding proxy. A lot of steps described here are skipped or run in a limited form when Relay is not running with processing enabled: Event normalization does different (less) things. In certain modes, project config is not fetched from Sentry at all (but rather from disk or filled out with defaults). Events are forwarded to an HTTP endpoint instead of being written to Kafka. Rate limits are not calculated using Redis, instead Relay just honors 429s from previously mentioned endpoint. Filters are not applied at all.","title":"Processing enabled vs not?"},{"location":"architecture/ingest-event-path/#inside-the-endpoint","text":"When an SDK hits /api/X/store on Relay, the code in server/src/endpoints/store.rs is called before returning a HTTP response. That code looks into an in-memory cache to answer basic information about a project such as: Does it exist? Is it suspended/disabled? Is it rate limited right now? If so, which key is rate limited? Which DSNs are valid for this project? Some of the data for this cache is coming from the projectconfigs endpoint . It is refreshed every couple of minutes, depending on configuration ( project_expiry ). If the cache is fresh, we may return a 429 for rate limits or a 4xx for invalid auth information. That cache might be empty or stale. If that is the case, Relay does not actually attempt to populate it at this stage. It just returns a 200 even though the event might be dropped later. This implies: The first store request that runs into a rate limit doesn't actually result in a 429 , but a subsequent request will (because by that time the project cache will have been updated). A store request for a non-existent project may result in a 200 , but subsequent ones will not. A store request with wrong auth information may result in a 200 , but subsequent ones will not. Filters are also not applied at this stage, so a filtered event will always result in a 200 . This matches the Python behavior since a while now . These examples assume that a project receives one event at a time. In practice one may observe that a highly concurrent burst of store requests for a single project results in 200 OK s only. However, a multi-second flood of incoming events should quickly result in eventually consistent and correct status codes. The response is completed at this point. All expensive work (such as talking to external services) is deferred into a background task. Except for responding to the HTTP request, there's no I/O done in the endpoint in any form. We didn't even hit Redis to calculate rate limits. Summary The HTTP response returned is just a best-effort guess at what the actual outcome of the event is going to be. We only return a 4xx code if we know that the response will fail (based on cached information), if we don't we return a 200 and continue to process the event asynchronously. This asynchronous processing used to happen synchronously in the Python implementation of StoreView . The effect of this is that the server will respond much faster that before but we might return 200 for events that will ultimately not be accepted. Generally Relay will return a 200 in many more situations than the old StoreView .","title":"Inside the endpoint"},{"location":"architecture/ingest-event-path/#the-background-task","text":"The HTTP response is out by now. The rest of what used to happen synchronously in the Python StoreView is done asynchronously, but still in the same process. So, now to the real work: Project config is fetched. If the project cache is stale or missing, we fetch it. We may wait a couple milliseconds ( batch_interval ) here to be able to batch multiple project config fetches into the same HTTP request to not overload Sentry too much. At this stage Relay may drop the event because it realized that the DSN was invalid or the project didn't even exist. The next incoming event will get a proper 4xx status code. The event is parsed. In the endpoint we only did decompression, a basic JSON syntax check, and extraction of the event ID to be able to return it as part of the response. Now we create an Event struct, which conceptually is the equivalent to parsing it into a Python dictionary: We allocate more memory. The event is normalized. Event normalization is probably the most CPU-intensive task running in Relay. It discards invalid data, moves data from deprecated fields to newer fields and generally just does schema validation. Filters (\"inbound filters\") are applied. Event may be discarded because of IP addresses, patterns on the error message or known web crawlers. Exact rate limits (\"quotas\") are applied. is_rate_limited.lua is executed on Redis. The input parameters for is_rate_limited.lua (\"quota objects\") are part of the project config. See this pull request for an explanation of what quota objects are. The event may be discarded here. If so, we write the rate limit info (reason and expiration timestamp) into the in-memory project cache so that the next store request returns a 429 in the endpoint and doesn't hit Redis at all. This contraption has the advantage that suspended or permanently rate-limited projects are very cheap to handle, and do not involve external services (ignoring the polling of the project config every couple of minutes). The event is datascrubbed. We have a PII config (new format) and a datascrubbing config (old format, converted to new format on the fly) as part of the project config fetched from Sentry. Event is written to Kafka. Note: If we discard an event at any point, an outcome is written to Kafka if Relay is configured to do so. Summary For events that returned a 200 we spawn an in-process background task that does the rest of what the old StoreView did. This task updates in-memory state for rate limits and disabled projects/keys.","title":"The background task"},{"location":"architecture/ingest-event-path/#the-outcomes-consumer","text":"Outcomes are small messages in Kafka that contain an event ID and information about whether that event was rejected, and if so, why. The outcomes consumer is mostly responsible for updating (user-visible) counters in Sentry (buffers/counters and tsdb, which are two separate systems).","title":"The outcomes consumer"},{"location":"architecture/ingest-event-path/#the-ingest-consumer","text":"The ingest consumer reads accepted events from Kafka, and also updates some stats. Some of those stats are billing-relevant. Its main purpose is to do what insert_data_to_database in Python store did: Call preprocess_event , after which comes sourcemap processing, native symbolication, grouping, snuba and all that other stuff that is of no concern to Relay.","title":"The ingest consumer"},{"location":"architecture/ingest-event-path/#sequence-diagram-of-components-inside-relay","text":"sequenceDiagram participant sdk as SDK participant endpoint as Endpoint participant projectcache as ProjectCache participant eventmanager as EventManager participant cpupool as CPU Pool sdk->>endpoint:POST /api/42/store activate endpoint endpoint->>projectcache: get project (cached only) activate projectcache projectcache-->>endpoint: return project deactivate projectcache Note over endpoint: Checking rate limits and auth (fast path) endpoint->>eventmanager: queue event activate eventmanager eventmanager-->>endpoint:event ID endpoint-->>sdk:200 OK deactivate endpoint eventmanager->>projectcache:fetch project activate projectcache Note over eventmanager,projectcache: web request (batched with other projects) projectcache-->>eventmanager: return project deactivate projectcache eventmanager->>cpupool: . activate cpupool Note over eventmanager,cpupool: normalization, datascrubbing, redis rate limits, ... cpupool-->>eventmanager: . deactivate cpupool Note over eventmanager: Send event to kafka deactivate eventmanager","title":"Sequence diagram of components inside Relay"},{"location":"architecture/project-configuration/","text":"Project configuration This document describes how Relay deals with project configurations. Overview Getting a project Here is how we obtain a Project actor ( used to get project configuration). Legend Redis Proj Cache = Redis Project Cache Upstream Source = Upstream Project Source sequenceDiagram participant extern as Some Actor participant projCache as Project Cache participant proj as Project participant redis as Redis Proj Cache participant upstream as Upstream Source Note over extern, upstream : Getting a Project returns a Project actor from the cache or simply creates a new Project actor and returns it extern->>projCache: GetProject(projId) activate projCache alt projId in ProjectCache projCache-->>extern: Project else projeId not found in cache projCache->>projCache: Project::new(projId, now) projCache-->>extern: Project end deactivate projCache Getting a project state Here is how we obtain project configuration. Legend Proj Cache = Project Cache Proj Loc Info = Project Local Info Redis Proj Cache = Redis Project Cache Upstream Source = Upstream Project Source sequenceDiagram participant extern as Some Actor participant projCache as Proj Cache participant local as Proj Loc Info participant proj as Project participant redis as Redis Proj Cache participant upstream as Upstream Source Note over extern, upstream : Fetching a Project state extern->>projCache: FetchProjectState(projId) activate projCache opt proj in cache Note right of projCache: Update last used projCache->>projCache: proj.last_update = now end opt projectLocal != None projCache ->> local: FetchOptionalProjectState(projId) activate local local -->> projCache: Option<ProjectState> deactivate local end alt projState != None projCache -->>extern: ProjectState else projState == None opt has redis cache projCache ->> redis: FetchOptionalProjectState(projId) activate redis redis -->> projCache: Option<ProjectState> deactivate redis end alt projState != None projCache -->> extern: ProjectState else projState == None projCache ->> upstream: FetchProjectState(projId) activate upstream upstream -->> extern: ProjectState deactivate upstream end end deactivate projCache Fetching the project state from Redis Fetching the project state from redis is straight forward. Relay does not do any sort of management of project states in Redis. From Relay's point of view a project state is either in Redis (and then it uses it as a result) or it isn't and then it looks for the project state in other places (upstream). If Relay obtains the project state from Upstream it will NOT insert it in Redis. It is up to other, external systems, to manage project states and add/remove/refresh them in Redis. Fetching the project state from Upstream If everything else fails and the ProjectCache can't obtain a project state from one of the chaches the Upstream will be queried for the ProjectState. Here's what happens in the Upstream actor Legend State Channel - Project State Channel Upstream Source - Upstream Project Source sequenceDiagram participant extern as Some Actor participant upstream as Upstream Source participant timer as Timer participant channel as State Channel participant http as Upstream Source extern->>upstream: FetchProjectState(projId) activate extern activate upstream upstream-->>timer: schedule_fetch() activate timer note right of upstream: Will return the <br> receiver of a <br> Project Channel upstream->>upstream: getOrCreateChannel(projId) note right of timer: at regular intervals timer->>upstream: fetch_states() deactivate timer activate upstream loop batch requests upstream->>http: GetProjectStates() activate http http->>upstream: GetProjectStatesResponse deactivate http end loop project states upstream->>channel: send(ProjectState) activate channel deactivate upstream end deactivate upstream channel->>extern: ProjectState deactivate channel deactivate extern .","title":"Project configuration"},{"location":"architecture/project-configuration/#project-configuration","text":"This document describes how Relay deals with project configurations.","title":"Project configuration"},{"location":"architecture/project-configuration/#overview","text":"","title":"Overview"},{"location":"architecture/project-configuration/#getting-a-project","text":"Here is how we obtain a Project actor ( used to get project configuration). Legend Redis Proj Cache = Redis Project Cache Upstream Source = Upstream Project Source sequenceDiagram participant extern as Some Actor participant projCache as Project Cache participant proj as Project participant redis as Redis Proj Cache participant upstream as Upstream Source Note over extern, upstream : Getting a Project returns a Project actor from the cache or simply creates a new Project actor and returns it extern->>projCache: GetProject(projId) activate projCache alt projId in ProjectCache projCache-->>extern: Project else projeId not found in cache projCache->>projCache: Project::new(projId, now) projCache-->>extern: Project end deactivate projCache","title":"Getting a project"},{"location":"architecture/project-configuration/#getting-a-project-state","text":"Here is how we obtain project configuration. Legend Proj Cache = Project Cache Proj Loc Info = Project Local Info Redis Proj Cache = Redis Project Cache Upstream Source = Upstream Project Source sequenceDiagram participant extern as Some Actor participant projCache as Proj Cache participant local as Proj Loc Info participant proj as Project participant redis as Redis Proj Cache participant upstream as Upstream Source Note over extern, upstream : Fetching a Project state extern->>projCache: FetchProjectState(projId) activate projCache opt proj in cache Note right of projCache: Update last used projCache->>projCache: proj.last_update = now end opt projectLocal != None projCache ->> local: FetchOptionalProjectState(projId) activate local local -->> projCache: Option<ProjectState> deactivate local end alt projState != None projCache -->>extern: ProjectState else projState == None opt has redis cache projCache ->> redis: FetchOptionalProjectState(projId) activate redis redis -->> projCache: Option<ProjectState> deactivate redis end alt projState != None projCache -->> extern: ProjectState else projState == None projCache ->> upstream: FetchProjectState(projId) activate upstream upstream -->> extern: ProjectState deactivate upstream end end deactivate projCache","title":"Getting a project state"},{"location":"architecture/project-configuration/#fetching-the-project-state-from-redis","text":"Fetching the project state from redis is straight forward. Relay does not do any sort of management of project states in Redis. From Relay's point of view a project state is either in Redis (and then it uses it as a result) or it isn't and then it looks for the project state in other places (upstream). If Relay obtains the project state from Upstream it will NOT insert it in Redis. It is up to other, external systems, to manage project states and add/remove/refresh them in Redis.","title":"Fetching the project state from Redis"},{"location":"architecture/project-configuration/#fetching-the-project-state-from-upstream","text":"If everything else fails and the ProjectCache can't obtain a project state from one of the chaches the Upstream will be queried for the ProjectState. Here's what happens in the Upstream actor Legend State Channel - Project State Channel Upstream Source - Upstream Project Source sequenceDiagram participant extern as Some Actor participant upstream as Upstream Source participant timer as Timer participant channel as State Channel participant http as Upstream Source extern->>upstream: FetchProjectState(projId) activate extern activate upstream upstream-->>timer: schedule_fetch() activate timer note right of upstream: Will return the <br> receiver of a <br> Project Channel upstream->>upstream: getOrCreateChannel(projId) note right of timer: at regular intervals timer->>upstream: fetch_states() deactivate timer activate upstream loop batch requests upstream->>http: GetProjectStates() activate http http->>upstream: GetProjectStatesResponse deactivate http end loop project states upstream->>channel: send(ProjectState) activate channel deactivate upstream end deactivate upstream channel->>extern: ProjectState deactivate channel deactivate extern .","title":"Fetching the project state from Upstream"},{"location":"event-schema/","text":"Event schema This page is intended to eventually replace our current event schema documentation . As opposed to the current one, this one is automatically generated from source code and therefore more likely to be up-to-date and exhaustive. It is still a work-in-progress. Right now we recommend using our existing docs as linked above and only fall back to this doc to resolve ambiguities. In addition to documentation the event schema is documented in machine-readable form: Download JSON schema (which is what this document is generated from) Event The sentry v7 event structure. Properties: breadcrumbs (optional): Breadcrumbs | null List of breadcrumbs recorded before this event. checksum (optional): null | string Legacy checksum used for grouping before fingerprint hashes. client_sdk (optional): ClientSDKInfo | null Information about the Sentry SDK that generated this event. contexts (optional): { [key: string]: null | Context } | null Contexts describing the environment (e.g. device, os or browser). culprit (optional): null | string Custom culprit of the event. debug_meta (optional): DebugMeta | null Meta data for event processing and debugging. dist (optional): null | string Program's distribution identifier. environment (optional): null | string Environment the environment was generated in (\"production\" or \"development\"). errors (optional): Array< EventProcessingError | null> | null Errors encountered during processing. Intended to be phased out in favor of annotation/metadata system. exceptions (optional): Exceptions | null One or multiple chained (nested) exceptions. extra (optional): { [key: string]: any } | null Arbitrary extra information set by the user. fingerprint (optional): string[] | null Manual fingerprint override. id (optional): null | string Unique identifier of this event. key_id (optional): null | string Project key which sent this event. level (optional): Level | null Severity level of the event. logentry (optional): LogEntry | null Custom parameterized message for this event. logger (optional): null | string Logger that created the event. modules (optional): { [key: string]: null | string } | null Name and versions of installed modules. platform (optional): null | string Platform identifier of this event (defaults to \"other\"). project (optional): number | null Project which sent this event. received (optional): number | null | string Timestamp when the event has been received by Sentry. release (optional): null | string Program's release identifier. request (optional): Request | null Information about a web request that occurred during the event. server_name (optional): null | string Server or device name the event was generated on. site (optional): null | string Deprecated in favor of tags. spans (optional): Array< Span | null> | null Spans for tracing. stacktrace (optional): Stacktrace | null Event stacktrace. DEPRECATED: Prefer threads or exception depending on which is more appropriate. start_timestamp (optional): number | null | string Timestamp when the event has started (relevant for event type = \"transaction\") tags (optional): Array<Array<null | string> | null> | { [key: string]: null | string } | null Custom tags for this event. threads (optional): Threads | null Threads that were active when the event occurred. time_spent (optional): number | null Time since the start of the transaction until the error occurred. timestamp (optional): number | null | string Timestamp when the event was created. transaction (optional): null | string Transaction name of the event. type (optional): EventType | null Type of event: error, csp, default user (optional): User | null Information about the user who triggered this event. version (optional): null | string Version Breadcrumbs Properties: values : Array< Breadcrumb | null> Breadcrumb A breadcrumb. Properties: category (optional): null | string The optional category of the breadcrumb. data (optional): { [key: string]: any } | null Custom user-defined data of this breadcrumb. level (optional): Level | null Severity level of the breadcrumb. message (optional): null | string Human readable message for the breadcrumb. timestamp (optional): number | null | string The timestamp of the breadcrumb. type (optional): null | string The type of the breadcrumb. Level Severity level of an event or breadcrumb. Variants: \"debug\" \"error\" \"fatal\" \"info\" \"warning\" ClientSDKInfo Information about the Sentry SDK. Properties: client_ip (optional): null | string IP Address of sender??? Seems unused. integrations (optional): Array<null | string> | null List of integrations that are enabled in the SDK. name (optional): null | string Unique SDK name. packages (optional): Array< ClientSDKPackage | null> | null List of installed and loaded SDK packages. version (optional): null | string SDK version. ClientSDKPackage An installed and loaded package as part of the Sentry SDK. Properties: name (optional): null | string Name of the package. version (optional): null | string Version of the package. Context Device information. Operating system information. Runtime information. Application information. Web browser information. Information about device's GPU. GPU information. Information related to Monitors feature. Trace context Monitor information. Additional arbitrary fields for forwards compatibility. Properties: arch (optional): null | string Native cpu architecture of the device. battery_level (optional): number | null Current battery level (0-100). boot_time (optional): null | string Indicator when the device was booted. brand (optional): null | string Brand of the device. charging (optional): boolean | null Whether the device was charging or not. external_free_storage (optional): number | null Free size of the attached external storage in bytes (eg: android SDK card). external_storage_size (optional): number | null Total size of the attached external storage in bytes (eg: android SDK card). family (optional): null | string Family of the device model. free_memory (optional): number | null How much memory is still available in bytes. free_storage (optional): number | null How much storage is free in bytes. low_memory (optional): boolean | null Whether the device was low on memory. manufacturer (optional): null | string Manufacturer of the device memory_size (optional): number | null Total memory available in bytes. model (optional): null | string Device model (human readable). model_id (optional): null | string Device model (internal identifier). name (optional): null | string Name of the device. Name of the operating system. Runtime name. online (optional): boolean | null Whether the device was online or not. orientation (optional): null | string Current screen orientation. screen_density (optional): number | null Device screen density. screen_dpi (optional): number | null Screen density as dots-per-inch. screen_resolution (optional): null | string Device screen resolution. simulator (optional): boolean | null Simulator/prod indicator. storage_size (optional): number | null Total storage size of the device in bytes. timezone (optional): null | string Timezone of the device. usable_memory (optional): number | null How much memory is usable for the app in bytes. build (optional): null | string Internal build number of the operating system. Application build string, if it is separate from the version. kernel_version (optional): null | string Current kernel version. raw_description (optional): null | string Unprocessed operating system info. Unprocessed runtime info. rooted (optional): boolean | null Indicator if the OS is rooted (mobile mostly). version (optional): null | string Version of the operating system. Runtime version string. Runtime version. app_build (optional): null | string Internal build ID as it appears on the platform. app_identifier (optional): null | string App identifier (dotted bundle id). app_name (optional): null | string Application name as it appears on the platform. app_start_time (optional): null | string Start time of the app. app_version (optional): null | string Application version as it appears on the platform. build_type (optional): null | string Build identicator. device_app_hash (optional): null | string Device app hash (app specific device ID) op (optional): null | string Span type (see OperationType docs). parent_span_id (optional): null | string The ID of the span enclosing this span. span_id (optional): null | string The ID of the span. status (optional): SpanStatus | null Whether the trace failed or succeeded. Currently only used to indicate status of individual transactions. trace_id (optional): null | string The trace ID. SpanStatus Trace status Values from https://github.com/open-telemetry/opentelemetry-specification/blob/8fb6c14e4709e75a9aaa64b0dbbdf02a6067682a/specification/api-tracing.md#status Mapping to HTTP from https://github.com/open-telemetry/opentelemetry-specification/blob/8fb6c14e4709e75a9aaa64b0dbbdf02a6067682a/specification/data-http.md#status Note: This type is represented as a u8 in Snuba/Clickhouse, with Unknown being the default value. We use repr(u8) to statically validate that the trace status has 255 variants at most. Variants: \"aborted\" \"already_exists\" \"cancelled\" \"data_loss\" \"deadline_exceeded\" \"failed_precondition\" \"internal_error\" \"invalid_argument\" \"not_found\" \"ok\" \"out_of_range\" \"permission_denied\" \"resource_exhausted\" \"unauthenticated\" \"unavailable\" \"unimplemented\" \"unknown\" DebugMeta Debugging and processing meta information. Properties: images (optional): Array<null | DebugImage > | null List of debug information files (debug images). system_sdk (optional): SystemSDKInfo | null Information about the system SDK (e.g. iOS SDK). DebugImage Legacy apple debug images (MachO). This was also used for non-apple platforms with similar debug setups. Apple debug image in Generic new style debug image. A native platform debug information file. MachO (macOS and iOS) debug image. ELF (Linux) debug image. PE (Windows) debug image. A reference to a proguard debug file. Proguard mapping file. A debug image that is unknown to this protocol specification. Properties: arch (optional): null | string CPU architecture target. cpu_subtype (optional): number | null MachO CPU subtype identifier. cpu_type (optional): number | null MachO CPU type identifier. image_addr (optional): null | string Starting memory address of the image (required). image_size (optional): number | null Size of the image in bytes (required). image_vmaddr (optional): null | string Loading address in virtual memory. name (optional): null | string Path and name of the debug image (required). uuid (optional): null | string The unique UUID of the image. UUID computed from the file contents. code_file (optional): null | string Path and name of the image file (required). code_id (optional): null | string Optional identifier of the code file. If not specified, it is assumed to be identical to the debug identifier. debug_file (optional): null | string Path and name of the debug companion file (required). debug_id (optional): null | string Unique debug identifier of the image. SystemSDKInfo Holds information about the system SDK. This is relevant for iOS and other platforms that have a system SDK. Not to be confused with the client SDK. Properties: sdk_name (optional): null | string The internal name of the SDK. version_major (optional): number | null The major version of the SDK as integer or 0. version_minor (optional): number | null The minor version of the SDK as integer or 0. version_patchlevel (optional): number | null The patch version of the SDK as integer or 0. EventProcessingError An event processing error. Properties: name (optional): null | string Affected key or deep path. type (optional): null | string The error kind. value (optional): any The original value causing this error. Exceptions Properties: values : Array< Exception | null> Exception A single exception. Properties: mechanism (optional): Mechanism | null Mechanism by which this exception was generated and handled. module (optional): null | string Module name of this exception. raw_stacktrace (optional): Stacktrace | null Optional unprocessed stack trace. stacktrace (optional): Stacktrace | null Stack trace containing frames of this exception. thread_id (optional): number | null | string Identifier of the thread this exception occurred in. type (optional): null | string Exception type. One of value or exception is required, checked in StoreNormalizeProcessor value (optional): null | string Human readable display value. Mechanism The mechanism by which an exception was generated and handled. Properties: data (optional): { [key: string]: any } | null Additional attributes depending on the mechanism type. description (optional): null | string Human readable detail description. handled (optional): boolean | null Flag indicating whether this exception was handled. help_link (optional): null | string Link to online resources describing this error. meta (optional): MechanismMeta | null Operating system or runtime meta information. synthetic (optional): boolean | null If this is set then the exception is not a real exception but some form of synthetic error for instance from a signal handler, a hard segfault or similar where type and value are not useful for grouping or display purposes. type (optional): null | string Mechanism type (required). MechanismMeta Operating system or runtime meta information to an exception mechanism. Properties: errno (optional): CError | null Optional ISO C standard error code. mach_exception (optional): MachException | null Optional mach exception information. signal (optional): POSIXSignal | null Optional POSIX signal number. CError POSIX signal with optional extended data. Properties: name (optional): null | string Optional name of the errno constant. number (optional): number | null The error code as specified by ISO C99, POSIX.1-2001 or POSIX.1-2008. MachException Mach exception information. Properties: code (optional): number | null The mach exception code. exception (optional): number | null The mach exception type. name (optional): null | string Optional name of the mach exception. subcode (optional): number | null The mach exception subcode. POSIXSignal POSIX signal with optional extended data. Properties: code (optional): number | null An optional signal code present on Apple systems. code_name (optional): null | string Optional name of the errno constant. name (optional): null | string Optional name of the errno constant. number (optional): number | null The POSIX signal number. Stacktrace Holds information about an entirey stacktrace. Newtype to distinguish raw_stacktrace attributes from the rest. Properties: frames (optional): Array< Frame | null> | null lang (optional): null | string The language of the stacktrace. registers (optional): { [key: string]: null | string } | null Register values of the thread (top frame). Frame Holds information about a single stacktrace frame. Properties: abs_path (optional): null | string Absolute path to the source file. colno (optional): number | null Column number within the source file. context_line (optional): null | string Source code of the current line. data (optional): FrameData | null Auxiliary information about the frame that is platform specific. filename (optional): null | string The source file name (basename only). function (optional): null | string Name of the frame's function. This might include the name of a class. image_addr (optional): null | string Start address of the containing code module (image). in_app (optional): boolean | null Override whether this frame should be considered in-app. instruction_addr (optional): null | string Absolute address of the frame's CPU instruction. lang (optional): null | string The language of the frame if it overrides the stacktrace language. lineno (optional): number | null Line number within the source file. module (optional): null | string Name of the module the frame is contained in. Note that this might also include a class name if that is something the language natively considers to be part of the stack (for instance in Java). package (optional): null | string Name of the package that contains the frame. For instance this can be a dylib for native languages, the name of the jar or .NET assembly. platform (optional): null | string Which platform this frame is from. post_context (optional): Array<null | string> | null Source code of the lines after the current line. pre_context (optional): Array<null | string> | null Source code leading up to the current line. raw_function (optional): null | string A raw (but potentially truncated) function value. If this has the same value as function it's best to be omitted. This exists because on many platforms the function itself contains additional information like overload specifies or a lot of generics which can make it exceed the maximum limit we provide for the field. In those cases then we cannot reliably trim down the function any more at a later point because the more valuable information has been removed. The logic to be applied is that an intelligently trimmed function name should be stored in function and the value before trimming is stored in this field instead. However also this field will be capped at 256 characters at the moment which often means that not the entire original value can be stored. symbol (optional): null | string Potentially mangled name of the symbol as it appears in an executable. This is different from a function name by generally being the mangled name that appears natively in the binary. This is relevant for languages like Swift, C++ or Rust. symbol_addr (optional): null | string Start address of the frame's function. trust (optional): null | string Used for native crashes to indicate how much we can \"trust\" the instruction_addr vars (optional): { [key: string]: any } | null Local variables in a convenient format. FrameData Additional frame data information. This value is set by the server and should not be set by the SDK. Properties: orig_colno (optional): number | null The original column number. orig_filename (optional): null | string The original minified filename. orig_function (optional): null | string The original function name before it was resolved. orig_in_app (optional): number | null The original value of the in_app flag before grouping enhancers ran. Because we need to handle more cases the following values are used: missing / null : information not available - -1 : in_app was set to null - 0 : in_app was set to false - 1 : in_app was set to true orig_lineno (optional): number | null The original line number. sourcemap (optional): null | string A reference to the sourcemap used. LogEntry A log entry message. A log message is similar to the message attribute on the event itself but can additionally hold optional parameters. Properties: formatted (optional): null | string The formatted message message (optional): null | string The log message with parameter placeholders. params (optional): any Positional parameters to be interpolated into the log message. Request Http request information. Properties: cookies (optional): Array<Array<null | string> | null> | { [key: string]: null | string } | null URL encoded contents of the Cookie header. data (optional): any Request data in any format that makes sense. env (optional): { [key: string]: any } | null Server environment data, such as CGI/WSGI. fragment (optional): null | string The fragment of the request URL. headers (optional): Array<Array<null | string> | null> | { [key: string]: null | string } | null HTTP request headers. inferred_content_type (optional): null | string The inferred content type of the request payload. method (optional): null | string HTTP request method. query_string (optional): Array<Array<null | string> | null> | { [key: string]: null | string } | null URL encoded HTTP query string. url (optional): null | string URL of the request. Span Properties: description : null | string Human readable description of a span (e.g. method URL). op : null | string Span type (see OperationType docs). parent_span_id : null | string The ID of the span enclosing this span. span_id : null | string The Span id. start_timestamp : number | null | string Timestamp when the span started. timestamp : number | null | string Timestamp when the span was ended. trace_id : null | string The ID of the trace the span belongs to. Threads Properties: values : Array< Thread | null> Thread A process thread of an event. Properties: crashed (optional): boolean | null Indicates that this thread requested the event (usually by crashing). current (optional): boolean | null Indicates that the thread was not suspended when the event was created. id (optional): number | null | string Identifier of this thread within the process (usually an integer). name (optional): null | string Display name of this thread. raw_stacktrace (optional): Stacktrace | null Optional unprocessed stack trace. stacktrace (optional): Stacktrace | null Stack trace containing frames of this exception. EventType The type of event we're dealing with. Variants: \"csp\" \"default\" \"error\" \"expectct\" \"expectstaple\" \"hpkp\" \"transaction\" User Information about the user who triggered an event. Properties: data (optional): { [key: string]: any } | null Additional arbitrary fields, as stored in the database (and sometimes as sent by clients). All data from self.other should end up here after store normalization. email (optional): null | string Email address of the user. geo (optional): Geo | null Approximate geographical location of the end user or device. id (optional): null | string Unique identifier of the user. ip_address (optional): null | string Remote IP address of the user. Defaults to \"\". name (optional): null | string Human readable name of the user. username (optional): null | string Username of the user. Geo Geographical location of the end user or device. Properties: city (optional): null | string Human readable city name. country_code (optional): null | string Two-letter country code (ISO 3166-1 alpha-2). region (optional): null | string Human readable region name or code.","title":"Event schema"},{"location":"event-schema/#event-schema","text":"This page is intended to eventually replace our current event schema documentation . As opposed to the current one, this one is automatically generated from source code and therefore more likely to be up-to-date and exhaustive. It is still a work-in-progress. Right now we recommend using our existing docs as linked above and only fall back to this doc to resolve ambiguities. In addition to documentation the event schema is documented in machine-readable form: Download JSON schema (which is what this document is generated from)","title":"Event schema"},{"location":"event-schema/#event","text":"The sentry v7 event structure. Properties: breadcrumbs (optional): Breadcrumbs | null List of breadcrumbs recorded before this event. checksum (optional): null | string Legacy checksum used for grouping before fingerprint hashes. client_sdk (optional): ClientSDKInfo | null Information about the Sentry SDK that generated this event. contexts (optional): { [key: string]: null | Context } | null Contexts describing the environment (e.g. device, os or browser). culprit (optional): null | string Custom culprit of the event. debug_meta (optional): DebugMeta | null Meta data for event processing and debugging. dist (optional): null | string Program's distribution identifier. environment (optional): null | string Environment the environment was generated in (\"production\" or \"development\"). errors (optional): Array< EventProcessingError | null> | null Errors encountered during processing. Intended to be phased out in favor of annotation/metadata system. exceptions (optional): Exceptions | null One or multiple chained (nested) exceptions. extra (optional): { [key: string]: any } | null Arbitrary extra information set by the user. fingerprint (optional): string[] | null Manual fingerprint override. id (optional): null | string Unique identifier of this event. key_id (optional): null | string Project key which sent this event. level (optional): Level | null Severity level of the event. logentry (optional): LogEntry | null Custom parameterized message for this event. logger (optional): null | string Logger that created the event. modules (optional): { [key: string]: null | string } | null Name and versions of installed modules. platform (optional): null | string Platform identifier of this event (defaults to \"other\"). project (optional): number | null Project which sent this event. received (optional): number | null | string Timestamp when the event has been received by Sentry. release (optional): null | string Program's release identifier. request (optional): Request | null Information about a web request that occurred during the event. server_name (optional): null | string Server or device name the event was generated on. site (optional): null | string Deprecated in favor of tags. spans (optional): Array< Span | null> | null Spans for tracing. stacktrace (optional): Stacktrace | null Event stacktrace. DEPRECATED: Prefer threads or exception depending on which is more appropriate. start_timestamp (optional): number | null | string Timestamp when the event has started (relevant for event type = \"transaction\") tags (optional): Array<Array<null | string> | null> | { [key: string]: null | string } | null Custom tags for this event. threads (optional): Threads | null Threads that were active when the event occurred. time_spent (optional): number | null Time since the start of the transaction until the error occurred. timestamp (optional): number | null | string Timestamp when the event was created. transaction (optional): null | string Transaction name of the event. type (optional): EventType | null Type of event: error, csp, default user (optional): User | null Information about the user who triggered this event. version (optional): null | string Version","title":"Event"},{"location":"event-schema/#breadcrumbs","text":"Properties: values : Array< Breadcrumb | null>","title":"Breadcrumbs"},{"location":"event-schema/#breadcrumb","text":"A breadcrumb. Properties: category (optional): null | string The optional category of the breadcrumb. data (optional): { [key: string]: any } | null Custom user-defined data of this breadcrumb. level (optional): Level | null Severity level of the breadcrumb. message (optional): null | string Human readable message for the breadcrumb. timestamp (optional): number | null | string The timestamp of the breadcrumb. type (optional): null | string The type of the breadcrumb.","title":"Breadcrumb"},{"location":"event-schema/#level","text":"Severity level of an event or breadcrumb. Variants: \"debug\" \"error\" \"fatal\" \"info\" \"warning\"","title":"Level"},{"location":"event-schema/#clientsdkinfo","text":"Information about the Sentry SDK. Properties: client_ip (optional): null | string IP Address of sender??? Seems unused. integrations (optional): Array<null | string> | null List of integrations that are enabled in the SDK. name (optional): null | string Unique SDK name. packages (optional): Array< ClientSDKPackage | null> | null List of installed and loaded SDK packages. version (optional): null | string SDK version.","title":"ClientSDKInfo"},{"location":"event-schema/#clientsdkpackage","text":"An installed and loaded package as part of the Sentry SDK. Properties: name (optional): null | string Name of the package. version (optional): null | string Version of the package.","title":"ClientSDKPackage"},{"location":"event-schema/#context","text":"Device information. Operating system information. Runtime information. Application information. Web browser information. Information about device's GPU. GPU information. Information related to Monitors feature. Trace context Monitor information. Additional arbitrary fields for forwards compatibility. Properties: arch (optional): null | string Native cpu architecture of the device. battery_level (optional): number | null Current battery level (0-100). boot_time (optional): null | string Indicator when the device was booted. brand (optional): null | string Brand of the device. charging (optional): boolean | null Whether the device was charging or not. external_free_storage (optional): number | null Free size of the attached external storage in bytes (eg: android SDK card). external_storage_size (optional): number | null Total size of the attached external storage in bytes (eg: android SDK card). family (optional): null | string Family of the device model. free_memory (optional): number | null How much memory is still available in bytes. free_storage (optional): number | null How much storage is free in bytes. low_memory (optional): boolean | null Whether the device was low on memory. manufacturer (optional): null | string Manufacturer of the device memory_size (optional): number | null Total memory available in bytes. model (optional): null | string Device model (human readable). model_id (optional): null | string Device model (internal identifier). name (optional): null | string Name of the device. Name of the operating system. Runtime name. online (optional): boolean | null Whether the device was online or not. orientation (optional): null | string Current screen orientation. screen_density (optional): number | null Device screen density. screen_dpi (optional): number | null Screen density as dots-per-inch. screen_resolution (optional): null | string Device screen resolution. simulator (optional): boolean | null Simulator/prod indicator. storage_size (optional): number | null Total storage size of the device in bytes. timezone (optional): null | string Timezone of the device. usable_memory (optional): number | null How much memory is usable for the app in bytes. build (optional): null | string Internal build number of the operating system. Application build string, if it is separate from the version. kernel_version (optional): null | string Current kernel version. raw_description (optional): null | string Unprocessed operating system info. Unprocessed runtime info. rooted (optional): boolean | null Indicator if the OS is rooted (mobile mostly). version (optional): null | string Version of the operating system. Runtime version string. Runtime version. app_build (optional): null | string Internal build ID as it appears on the platform. app_identifier (optional): null | string App identifier (dotted bundle id). app_name (optional): null | string Application name as it appears on the platform. app_start_time (optional): null | string Start time of the app. app_version (optional): null | string Application version as it appears on the platform. build_type (optional): null | string Build identicator. device_app_hash (optional): null | string Device app hash (app specific device ID) op (optional): null | string Span type (see OperationType docs). parent_span_id (optional): null | string The ID of the span enclosing this span. span_id (optional): null | string The ID of the span. status (optional): SpanStatus | null Whether the trace failed or succeeded. Currently only used to indicate status of individual transactions. trace_id (optional): null | string The trace ID.","title":"Context"},{"location":"event-schema/#spanstatus","text":"Trace status Values from https://github.com/open-telemetry/opentelemetry-specification/blob/8fb6c14e4709e75a9aaa64b0dbbdf02a6067682a/specification/api-tracing.md#status Mapping to HTTP from https://github.com/open-telemetry/opentelemetry-specification/blob/8fb6c14e4709e75a9aaa64b0dbbdf02a6067682a/specification/data-http.md#status Note: This type is represented as a u8 in Snuba/Clickhouse, with Unknown being the default value. We use repr(u8) to statically validate that the trace status has 255 variants at most. Variants: \"aborted\" \"already_exists\" \"cancelled\" \"data_loss\" \"deadline_exceeded\" \"failed_precondition\" \"internal_error\" \"invalid_argument\" \"not_found\" \"ok\" \"out_of_range\" \"permission_denied\" \"resource_exhausted\" \"unauthenticated\" \"unavailable\" \"unimplemented\" \"unknown\"","title":"SpanStatus"},{"location":"event-schema/#debugmeta","text":"Debugging and processing meta information. Properties: images (optional): Array<null | DebugImage > | null List of debug information files (debug images). system_sdk (optional): SystemSDKInfo | null Information about the system SDK (e.g. iOS SDK).","title":"DebugMeta"},{"location":"event-schema/#debugimage","text":"Legacy apple debug images (MachO). This was also used for non-apple platforms with similar debug setups. Apple debug image in Generic new style debug image. A native platform debug information file. MachO (macOS and iOS) debug image. ELF (Linux) debug image. PE (Windows) debug image. A reference to a proguard debug file. Proguard mapping file. A debug image that is unknown to this protocol specification. Properties: arch (optional): null | string CPU architecture target. cpu_subtype (optional): number | null MachO CPU subtype identifier. cpu_type (optional): number | null MachO CPU type identifier. image_addr (optional): null | string Starting memory address of the image (required). image_size (optional): number | null Size of the image in bytes (required). image_vmaddr (optional): null | string Loading address in virtual memory. name (optional): null | string Path and name of the debug image (required). uuid (optional): null | string The unique UUID of the image. UUID computed from the file contents. code_file (optional): null | string Path and name of the image file (required). code_id (optional): null | string Optional identifier of the code file. If not specified, it is assumed to be identical to the debug identifier. debug_file (optional): null | string Path and name of the debug companion file (required). debug_id (optional): null | string Unique debug identifier of the image.","title":"DebugImage"},{"location":"event-schema/#systemsdkinfo","text":"Holds information about the system SDK. This is relevant for iOS and other platforms that have a system SDK. Not to be confused with the client SDK. Properties: sdk_name (optional): null | string The internal name of the SDK. version_major (optional): number | null The major version of the SDK as integer or 0. version_minor (optional): number | null The minor version of the SDK as integer or 0. version_patchlevel (optional): number | null The patch version of the SDK as integer or 0.","title":"SystemSDKInfo"},{"location":"event-schema/#eventprocessingerror","text":"An event processing error. Properties: name (optional): null | string Affected key or deep path. type (optional): null | string The error kind. value (optional): any The original value causing this error.","title":"EventProcessingError"},{"location":"event-schema/#exceptions","text":"Properties: values : Array< Exception | null>","title":"Exceptions"},{"location":"event-schema/#exception","text":"A single exception. Properties: mechanism (optional): Mechanism | null Mechanism by which this exception was generated and handled. module (optional): null | string Module name of this exception. raw_stacktrace (optional): Stacktrace | null Optional unprocessed stack trace. stacktrace (optional): Stacktrace | null Stack trace containing frames of this exception. thread_id (optional): number | null | string Identifier of the thread this exception occurred in. type (optional): null | string Exception type. One of value or exception is required, checked in StoreNormalizeProcessor value (optional): null | string Human readable display value.","title":"Exception"},{"location":"event-schema/#mechanism","text":"The mechanism by which an exception was generated and handled. Properties: data (optional): { [key: string]: any } | null Additional attributes depending on the mechanism type. description (optional): null | string Human readable detail description. handled (optional): boolean | null Flag indicating whether this exception was handled. help_link (optional): null | string Link to online resources describing this error. meta (optional): MechanismMeta | null Operating system or runtime meta information. synthetic (optional): boolean | null If this is set then the exception is not a real exception but some form of synthetic error for instance from a signal handler, a hard segfault or similar where type and value are not useful for grouping or display purposes. type (optional): null | string Mechanism type (required).","title":"Mechanism"},{"location":"event-schema/#mechanismmeta","text":"Operating system or runtime meta information to an exception mechanism. Properties: errno (optional): CError | null Optional ISO C standard error code. mach_exception (optional): MachException | null Optional mach exception information. signal (optional): POSIXSignal | null Optional POSIX signal number.","title":"MechanismMeta"},{"location":"event-schema/#cerror","text":"POSIX signal with optional extended data. Properties: name (optional): null | string Optional name of the errno constant. number (optional): number | null The error code as specified by ISO C99, POSIX.1-2001 or POSIX.1-2008.","title":"CError"},{"location":"event-schema/#machexception","text":"Mach exception information. Properties: code (optional): number | null The mach exception code. exception (optional): number | null The mach exception type. name (optional): null | string Optional name of the mach exception. subcode (optional): number | null The mach exception subcode.","title":"MachException"},{"location":"event-schema/#posixsignal","text":"POSIX signal with optional extended data. Properties: code (optional): number | null An optional signal code present on Apple systems. code_name (optional): null | string Optional name of the errno constant. name (optional): null | string Optional name of the errno constant. number (optional): number | null The POSIX signal number.","title":"POSIXSignal"},{"location":"event-schema/#stacktrace","text":"Holds information about an entirey stacktrace. Newtype to distinguish raw_stacktrace attributes from the rest. Properties: frames (optional): Array< Frame | null> | null lang (optional): null | string The language of the stacktrace. registers (optional): { [key: string]: null | string } | null Register values of the thread (top frame).","title":"Stacktrace"},{"location":"event-schema/#frame","text":"Holds information about a single stacktrace frame. Properties: abs_path (optional): null | string Absolute path to the source file. colno (optional): number | null Column number within the source file. context_line (optional): null | string Source code of the current line. data (optional): FrameData | null Auxiliary information about the frame that is platform specific. filename (optional): null | string The source file name (basename only). function (optional): null | string Name of the frame's function. This might include the name of a class. image_addr (optional): null | string Start address of the containing code module (image). in_app (optional): boolean | null Override whether this frame should be considered in-app. instruction_addr (optional): null | string Absolute address of the frame's CPU instruction. lang (optional): null | string The language of the frame if it overrides the stacktrace language. lineno (optional): number | null Line number within the source file. module (optional): null | string Name of the module the frame is contained in. Note that this might also include a class name if that is something the language natively considers to be part of the stack (for instance in Java). package (optional): null | string Name of the package that contains the frame. For instance this can be a dylib for native languages, the name of the jar or .NET assembly. platform (optional): null | string Which platform this frame is from. post_context (optional): Array<null | string> | null Source code of the lines after the current line. pre_context (optional): Array<null | string> | null Source code leading up to the current line. raw_function (optional): null | string A raw (but potentially truncated) function value. If this has the same value as function it's best to be omitted. This exists because on many platforms the function itself contains additional information like overload specifies or a lot of generics which can make it exceed the maximum limit we provide for the field. In those cases then we cannot reliably trim down the function any more at a later point because the more valuable information has been removed. The logic to be applied is that an intelligently trimmed function name should be stored in function and the value before trimming is stored in this field instead. However also this field will be capped at 256 characters at the moment which often means that not the entire original value can be stored. symbol (optional): null | string Potentially mangled name of the symbol as it appears in an executable. This is different from a function name by generally being the mangled name that appears natively in the binary. This is relevant for languages like Swift, C++ or Rust. symbol_addr (optional): null | string Start address of the frame's function. trust (optional): null | string Used for native crashes to indicate how much we can \"trust\" the instruction_addr vars (optional): { [key: string]: any } | null Local variables in a convenient format.","title":"Frame"},{"location":"event-schema/#framedata","text":"Additional frame data information. This value is set by the server and should not be set by the SDK. Properties: orig_colno (optional): number | null The original column number. orig_filename (optional): null | string The original minified filename. orig_function (optional): null | string The original function name before it was resolved. orig_in_app (optional): number | null The original value of the in_app flag before grouping enhancers ran. Because we need to handle more cases the following values are used: missing / null : information not available - -1 : in_app was set to null - 0 : in_app was set to false - 1 : in_app was set to true orig_lineno (optional): number | null The original line number. sourcemap (optional): null | string A reference to the sourcemap used.","title":"FrameData"},{"location":"event-schema/#logentry","text":"A log entry message. A log message is similar to the message attribute on the event itself but can additionally hold optional parameters. Properties: formatted (optional): null | string The formatted message message (optional): null | string The log message with parameter placeholders. params (optional): any Positional parameters to be interpolated into the log message.","title":"LogEntry"},{"location":"event-schema/#request","text":"Http request information. Properties: cookies (optional): Array<Array<null | string> | null> | { [key: string]: null | string } | null URL encoded contents of the Cookie header. data (optional): any Request data in any format that makes sense. env (optional): { [key: string]: any } | null Server environment data, such as CGI/WSGI. fragment (optional): null | string The fragment of the request URL. headers (optional): Array<Array<null | string> | null> | { [key: string]: null | string } | null HTTP request headers. inferred_content_type (optional): null | string The inferred content type of the request payload. method (optional): null | string HTTP request method. query_string (optional): Array<Array<null | string> | null> | { [key: string]: null | string } | null URL encoded HTTP query string. url (optional): null | string URL of the request.","title":"Request"},{"location":"event-schema/#span","text":"Properties: description : null | string Human readable description of a span (e.g. method URL). op : null | string Span type (see OperationType docs). parent_span_id : null | string The ID of the span enclosing this span. span_id : null | string The Span id. start_timestamp : number | null | string Timestamp when the span started. timestamp : number | null | string Timestamp when the span was ended. trace_id : null | string The ID of the trace the span belongs to.","title":"Span"},{"location":"event-schema/#threads","text":"Properties: values : Array< Thread | null>","title":"Threads"},{"location":"event-schema/#thread","text":"A process thread of an event. Properties: crashed (optional): boolean | null Indicates that this thread requested the event (usually by crashing). current (optional): boolean | null Indicates that the thread was not suspended when the event was created. id (optional): number | null | string Identifier of this thread within the process (usually an integer). name (optional): null | string Display name of this thread. raw_stacktrace (optional): Stacktrace | null Optional unprocessed stack trace. stacktrace (optional): Stacktrace | null Stack trace containing frames of this exception.","title":"Thread"},{"location":"event-schema/#eventtype","text":"The type of event we're dealing with. Variants: \"csp\" \"default\" \"error\" \"expectct\" \"expectstaple\" \"hpkp\" \"transaction\"","title":"EventType"},{"location":"event-schema/#user","text":"Information about the user who triggered an event. Properties: data (optional): { [key: string]: any } | null Additional arbitrary fields, as stored in the database (and sometimes as sent by clients). All data from self.other should end up here after store normalization. email (optional): null | string Email address of the user. geo (optional): Geo | null Approximate geographical location of the end user or device. id (optional): null | string Unique identifier of the user. ip_address (optional): null | string Remote IP address of the user. Defaults to \"\". name (optional): null | string Human readable name of the user. username (optional): null | string Username of the user.","title":"User"},{"location":"event-schema/#geo","text":"Geographical location of the end user or device. Properties: city (optional): null | string Human readable city name. country_code (optional): null | string Two-letter country code (ISO 3166-1 alpha-2). region (optional): null | string Human readable region name or code.","title":"Geo"},{"location":"pii-config/","text":"PII Configuration The following document explores the syntax and semantics of the new datascrubbing (\"PII\") configuration consumed by Relay . This type of configuration is supposed to eventually replace our existing server-side data scrubbing feature . We are currently beta-testing this feature on sentry.io . If you are interested in gaining early access to this feature, please refer to this issue or contact us at markus@sentry.io . A basic example Say you have an exception message which, unfortunately, contains IP addresses which are not supposed to be there. You'd write: { \"applications\" : { \"$string\" : [ \"@ip:replace\" ] } } It reads as \"replace all IP addresses in all strings\", or \"apply @ip:replace to all $string fields\". @ip:replace is called a rule, and $string is a selector . Built-in rules The following rules exist by default: @ip:replace and @ip:hash for replacing IP addresses. @imei:replace and @imei:hash for replacing IMEIs @mac:replace , @mac:mask and @mac:hash for matching MAC addresses @email:mask , @email:replace and @email:hash for matching email addresses @creditcard:mask , @creditcard:replace and @creditcard:hash for matching creditcard numbers @userpath:replace and @userpath:hash for matching local paths (e.g. C:/Users/foo/ ) @password:remove for removing passwords. In this case we're pattern matching against the field's key, whether it contains password , credentials or similar strings. @anything:remove , @anything:replace and @anything:hash for removing, replacing or hashing any value. It is essentially equivalent to a wildcard-regex, but it will also match much more than strings. Writing your own rules Rules generally consist of two parts: Rule types describe what to match. See PII Rule Types for an exhaustive list. Rule redaction methods describe what to do with the match. See PII Redaction Methods for a list. Each page comes with examples. Try those examples out by pasting them into the \"PII config\" column of Piinguin and clicking on fields to get suggestions.","title":"PII Configuration"},{"location":"pii-config/#pii-configuration","text":"The following document explores the syntax and semantics of the new datascrubbing (\"PII\") configuration consumed by Relay . This type of configuration is supposed to eventually replace our existing server-side data scrubbing feature . We are currently beta-testing this feature on sentry.io . If you are interested in gaining early access to this feature, please refer to this issue or contact us at markus@sentry.io .","title":"PII Configuration"},{"location":"pii-config/#a-basic-example","text":"Say you have an exception message which, unfortunately, contains IP addresses which are not supposed to be there. You'd write: { \"applications\" : { \"$string\" : [ \"@ip:replace\" ] } } It reads as \"replace all IP addresses in all strings\", or \"apply @ip:replace to all $string fields\". @ip:replace is called a rule, and $string is a selector .","title":"A basic example"},{"location":"pii-config/#built-in-rules","text":"The following rules exist by default: @ip:replace and @ip:hash for replacing IP addresses. @imei:replace and @imei:hash for replacing IMEIs @mac:replace , @mac:mask and @mac:hash for matching MAC addresses @email:mask , @email:replace and @email:hash for matching email addresses @creditcard:mask , @creditcard:replace and @creditcard:hash for matching creditcard numbers @userpath:replace and @userpath:hash for matching local paths (e.g. C:/Users/foo/ ) @password:remove for removing passwords. In this case we're pattern matching against the field's key, whether it contains password , credentials or similar strings. @anything:remove , @anything:replace and @anything:hash for removing, replacing or hashing any value. It is essentially equivalent to a wildcard-regex, but it will also match much more than strings.","title":"Built-in rules"},{"location":"pii-config/#writing-your-own-rules","text":"Rules generally consist of two parts: Rule types describe what to match. See PII Rule Types for an exhaustive list. Rule redaction methods describe what to do with the match. See PII Redaction Methods for a list. Each page comes with examples. Try those examples out by pasting them into the \"PII config\" column of Piinguin and clicking on fields to get suggestions.","title":"Writing your own rules"},{"location":"pii-config/methods/","text":"PII Redaction Methods remove Remove the entire field. Relay may choose to either set it to null or to remove it entirely. { \"rules\" : { \"remove_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"$string\" : [ \"remove_ip\" ] } } replace Replace the key with a static string. { \"rules\" : { \"replace_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"replace\" , \"text\" : [ censored ] \" } } }, \" applications \": { \" $string \": [\" replace_ip\"] } } mask Replace every character of the matched string with a \"masking\" char. Compared to replace this preserves the length of the original string. { \"rules\" : { \"mask_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"mask\" , \"mask_char\" : \"0\" , // The character used for masking. Optional, default \"*\" \"chars_to_ignore\" : \".\" , // Which characters to ignore. Optional, default empty \"range\" : [ 0 , - 1 ] // Which range of the string to replace. Optional, defaults to full range. Negative indices count from the matches' end. } } }, \"applications\" : { \"$string\" : [ \"mask_ip\" ] } } hash Replace the string with a hashed version of itself. Equal strings will produce the same hash, so if you, for example, decide to hash the user ID instead of replacing or removing it, you will still have an accurate count of users affected. { \"rules\" : { \"hash_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"hash\" , \"algorithm\" : \"HMAC-SHA1\" , // One of \"HMAC-SHA1\", \"HMAC-SHA256\", \"HMAC-SHA512\" \"key\" : \"myOverriddenKey\" // A key to salt the hash with. Defaults to the default key set in \"vars\" } } }, \"vars\" : { \"hashKey\" : \"myDefaultKey\" // The default key to use } \"applications\" : { \"$string\" : [ \"mask_ip\" ] } }","title":"PII Redaction Methods"},{"location":"pii-config/methods/#pii-redaction-methods","text":"","title":"PII Redaction Methods"},{"location":"pii-config/methods/#remove","text":"Remove the entire field. Relay may choose to either set it to null or to remove it entirely. { \"rules\" : { \"remove_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"$string\" : [ \"remove_ip\" ] } }","title":"remove"},{"location":"pii-config/methods/#replace","text":"Replace the key with a static string. { \"rules\" : { \"replace_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"replace\" , \"text\" : [ censored ] \" } } }, \" applications \": { \" $string \": [\" replace_ip\"] } }","title":"replace"},{"location":"pii-config/methods/#mask","text":"Replace every character of the matched string with a \"masking\" char. Compared to replace this preserves the length of the original string. { \"rules\" : { \"mask_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"mask\" , \"mask_char\" : \"0\" , // The character used for masking. Optional, default \"*\" \"chars_to_ignore\" : \".\" , // Which characters to ignore. Optional, default empty \"range\" : [ 0 , - 1 ] // Which range of the string to replace. Optional, defaults to full range. Negative indices count from the matches' end. } } }, \"applications\" : { \"$string\" : [ \"mask_ip\" ] } }","title":"mask"},{"location":"pii-config/methods/#hash","text":"Replace the string with a hashed version of itself. Equal strings will produce the same hash, so if you, for example, decide to hash the user ID instead of replacing or removing it, you will still have an accurate count of users affected. { \"rules\" : { \"hash_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"hash\" , \"algorithm\" : \"HMAC-SHA1\" , // One of \"HMAC-SHA1\", \"HMAC-SHA256\", \"HMAC-SHA512\" \"key\" : \"myOverriddenKey\" // A key to salt the hash with. Defaults to the default key set in \"vars\" } } }, \"vars\" : { \"hashKey\" : \"myDefaultKey\" // The default key to use } \"applications\" : { \"$string\" : [ \"mask_ip\" ] } }","title":"hash"},{"location":"pii-config/selectors/","text":"PII Selectors Selectors allow you to restrict rules to certain parts of the event. This is useful to unconditionally remove certain data by variable/field name from the event, but can also be used to conservatively test rules on real data. Data scrubbing always works on the raw event payload. Keep in mind that some fields in the UI may be called differently in the JSON schema. When looking at an event there should always be a link called \"JSON\" present that allows you to see what the data scrubber sees. For example, what is called \"Additional Data\" in the UI is called extra in the event payload. To remove a specific key called foo , you would write: [ Remove ] [ Anything ] from [ extra.foo ] Another example. Sentry knows about two kinds of error messages: The exception message, and the top-level log message. Here is an example of how such an event payload as sent by the SDK (and downloadable from the UI) would look like: { \"logentry\" : { \"formatted\" : \"Failed to roll out the dinglebop\" }, \"exceptions\" : { \"values\" : [ { \"type\" : \"ZeroDivisionError\" , \"value\" : \"integer division or modulo by zero\" , } ] } } Since the \"error message\" is taken from the exception 's value , and the \"message\" is taken from logentry , we would have to write the following to remove both from the event: [Remove] [Anything] from [exception.value] [Remove] [Anything] from [logentry.formatted] Boolean Logic You can combine selectors using boolean logic. Prefix with ! to invert the selector. foo matches the JSON key foo , while !foo matches everything but foo . Build the conjunction (AND) using && , such as: foo && !extra.foo to match the key foo except when inside of extra . Build the disjunction (OR) using || , such as: foo || bar to match foo or bar . Wildcards ** matches all subpaths, so that foo.** matches all JSON keys within foo . * matches a single path item, so that foo.* matches all JSON keys one level below foo . Value Types Select subsections by JSON-type using the following: $string matches any string value $number matches any integer or float value $datetime matches any field in the event that represents a timestamp $array matches any JSON array value $object matches any JSON object Select known parts of the schema using the following: $exception matches a single exception instance in {\"exception\": {\"values\": [...]}} $stacktrace matches a stack trace instance $frame matches a frame $request matches the HTTP request context of an event $user matches the user context of an event $logentry (also applies to the message attribute) $thread matches a single thread instance in {\"threads\": {\"values\": [...]}} $breadcrumb matches a single breadcrumb in {\"breadcrumbs\": [...]} $span matches a trace span $sdk matches the SDK context in {\"sdk\": ...} Examples Delete event.user : [ Remove ] [ Anything ] from [ $user ] Delete all frame-local variables: [ Remove ] [ Anything ] from [ $frame.vars ] Escaping Specal Characters If the object key you want to match contains whitespace or special characters, you can use quotes to escape it: [ Remove ] [ Anything ] from [ extra.'my special value' ] This matches the key my special value in Additional Data . To escape ' (single quote) within the quotes, replace it with '' (two quotes): [ Remove ] [ Anything ] from [ extra.'my special '' value' ] This matches the key my special ' value in Additional Data .","title":"PII Selectors"},{"location":"pii-config/selectors/#pii-selectors","text":"Selectors allow you to restrict rules to certain parts of the event. This is useful to unconditionally remove certain data by variable/field name from the event, but can also be used to conservatively test rules on real data. Data scrubbing always works on the raw event payload. Keep in mind that some fields in the UI may be called differently in the JSON schema. When looking at an event there should always be a link called \"JSON\" present that allows you to see what the data scrubber sees. For example, what is called \"Additional Data\" in the UI is called extra in the event payload. To remove a specific key called foo , you would write: [ Remove ] [ Anything ] from [ extra.foo ] Another example. Sentry knows about two kinds of error messages: The exception message, and the top-level log message. Here is an example of how such an event payload as sent by the SDK (and downloadable from the UI) would look like: { \"logentry\" : { \"formatted\" : \"Failed to roll out the dinglebop\" }, \"exceptions\" : { \"values\" : [ { \"type\" : \"ZeroDivisionError\" , \"value\" : \"integer division or modulo by zero\" , } ] } } Since the \"error message\" is taken from the exception 's value , and the \"message\" is taken from logentry , we would have to write the following to remove both from the event: [Remove] [Anything] from [exception.value] [Remove] [Anything] from [logentry.formatted]","title":"PII Selectors"},{"location":"pii-config/selectors/#boolean-logic","text":"You can combine selectors using boolean logic. Prefix with ! to invert the selector. foo matches the JSON key foo , while !foo matches everything but foo . Build the conjunction (AND) using && , such as: foo && !extra.foo to match the key foo except when inside of extra . Build the disjunction (OR) using || , such as: foo || bar to match foo or bar .","title":"Boolean Logic"},{"location":"pii-config/selectors/#wildcards","text":"** matches all subpaths, so that foo.** matches all JSON keys within foo . * matches a single path item, so that foo.* matches all JSON keys one level below foo .","title":"Wildcards"},{"location":"pii-config/selectors/#value-types","text":"Select subsections by JSON-type using the following: $string matches any string value $number matches any integer or float value $datetime matches any field in the event that represents a timestamp $array matches any JSON array value $object matches any JSON object Select known parts of the schema using the following: $exception matches a single exception instance in {\"exception\": {\"values\": [...]}} $stacktrace matches a stack trace instance $frame matches a frame $request matches the HTTP request context of an event $user matches the user context of an event $logentry (also applies to the message attribute) $thread matches a single thread instance in {\"threads\": {\"values\": [...]}} $breadcrumb matches a single breadcrumb in {\"breadcrumbs\": [...]} $span matches a trace span $sdk matches the SDK context in {\"sdk\": ...}","title":"Value Types"},{"location":"pii-config/selectors/#examples","text":"Delete event.user : [ Remove ] [ Anything ] from [ $user ] Delete all frame-local variables: [ Remove ] [ Anything ] from [ $frame.vars ]","title":"Examples"},{"location":"pii-config/selectors/#escaping-specal-characters","text":"If the object key you want to match contains whitespace or special characters, you can use quotes to escape it: [ Remove ] [ Anything ] from [ extra.'my special value' ] This matches the key my special value in Additional Data . To escape ' (single quote) within the quotes, replace it with '' (two quotes): [ Remove ] [ Anything ] from [ extra.'my special '' value' ] This matches the key my special ' value in Additional Data .","title":"Escaping Specal Characters"},{"location":"pii-config/types/","text":"PII Rule Types pattern Custom Perl-style regex (PCRE). { \"rules\" : { \"hash_device_id\" : { \"type\" : \"pattern\" , \"pattern\" : \"d/[a-f0-9]{12}\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_device_id\" ] } } imei Matches an IMEI or IMEISV. { \"rules\" : { \"hash_imei\" : { \"type\" : \"imei\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_imei\" ] } } mac Matches a MAC address. { \"rules\" : { \"hash_mac\" : { \"type\" : \"mac\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_mac\" ] } } ip Matches any IP address. { \"rules\" : { \"hash_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_ip\" ] } } creditcard Matches a creditcard number. { \"rules\" : { \"hash_cc\" : { \"type\" : \"creditcard\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_cc\" ] } } userpath Matches a local path (e.g. C:/Users/foo/ ). { \"rules\" : { \"hash_userpath\" : { \"type\" : \"userpath\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_userpath\" ] } } anything Matches any value. This is basically equivalent to a wildcard regex. For example, to remove all strings: { \"rules\" : { \"remove_everything\" : { \"type\" : \"anything\" , \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"$string\" : [ \"remove_everything\" ] } } multiple Combine multiple rules into one. This is a disjunction (OR): The field in question has to match only one of the rules to match the combined rule, not all of them. { \"rules\" : { \"remove_ips_and_macs\" : { \"type\" : \"multiple\" , \"rules\" : [ \"@ip\" , \"@mac\" ], \"hide_rule\" : false , // Hide the inner rules when showing which rules have been applied. Defaults to false. \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"$string\" : [ \"remove_ips_and_macs\" ] } } alias Alias one rule to the other. This is the same as multiple except that you can only wrap one rule. { \"rules\" : { \"remove_ips\" : { \"type\" : \"multiple\" , \"rule\" : \"@ip\" , \"hide_rule\" : false , // Hide the inner rule when showing which rules have been applied. Defaults to false. \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"$string\" : [ \"remove_ips\" ] } }","title":"PII Rule Types"},{"location":"pii-config/types/#pii-rule-types","text":"","title":"PII Rule Types"},{"location":"pii-config/types/#pattern","text":"Custom Perl-style regex (PCRE). { \"rules\" : { \"hash_device_id\" : { \"type\" : \"pattern\" , \"pattern\" : \"d/[a-f0-9]{12}\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_device_id\" ] } }","title":"pattern"},{"location":"pii-config/types/#imei","text":"Matches an IMEI or IMEISV. { \"rules\" : { \"hash_imei\" : { \"type\" : \"imei\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_imei\" ] } }","title":"imei"},{"location":"pii-config/types/#mac","text":"Matches a MAC address. { \"rules\" : { \"hash_mac\" : { \"type\" : \"mac\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_mac\" ] } }","title":"mac"},{"location":"pii-config/types/#ip","text":"Matches any IP address. { \"rules\" : { \"hash_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_ip\" ] } }","title":"ip"},{"location":"pii-config/types/#creditcard","text":"Matches a creditcard number. { \"rules\" : { \"hash_cc\" : { \"type\" : \"creditcard\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_cc\" ] } }","title":"creditcard"},{"location":"pii-config/types/#userpath","text":"Matches a local path (e.g. C:/Users/foo/ ). { \"rules\" : { \"hash_userpath\" : { \"type\" : \"userpath\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"$string\" : [ \"hash_userpath\" ] } }","title":"userpath"},{"location":"pii-config/types/#anything","text":"Matches any value. This is basically equivalent to a wildcard regex. For example, to remove all strings: { \"rules\" : { \"remove_everything\" : { \"type\" : \"anything\" , \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"$string\" : [ \"remove_everything\" ] } }","title":"anything"},{"location":"pii-config/types/#multiple","text":"Combine multiple rules into one. This is a disjunction (OR): The field in question has to match only one of the rules to match the combined rule, not all of them. { \"rules\" : { \"remove_ips_and_macs\" : { \"type\" : \"multiple\" , \"rules\" : [ \"@ip\" , \"@mac\" ], \"hide_rule\" : false , // Hide the inner rules when showing which rules have been applied. Defaults to false. \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"$string\" : [ \"remove_ips_and_macs\" ] } }","title":"multiple"},{"location":"pii-config/types/#alias","text":"Alias one rule to the other. This is the same as multiple except that you can only wrap one rule. { \"rules\" : { \"remove_ips\" : { \"type\" : \"multiple\" , \"rule\" : \"@ip\" , \"hide_rule\" : false , // Hide the inner rule when showing which rules have been applied. Defaults to false. \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"$string\" : [ \"remove_ips\" ] } }","title":"alias"}]}