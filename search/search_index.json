{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The Sentry Relay (aka Semaphore) is a work in progress service that pushes some functionality from the Sentry SDKs as well as the Sentry server into a proxy process. Getting started The relay server is called \"semaphore\". Binaries can be downloaded from the GitHub releases page . After downloading place the binary somewhere on your PATH and make it executable. The config init command is provided to initialize the initial config. The config will be created the folder it's run from in a hidden .semaphore subdirectory: $ semaphore config init The wizard will ask a few questions: default vs custom config. In the default config the relay will connect to sentry.io for sending and will generally use the defaults. In the custom config a different upstream can be configured. Relay internal crash reporting can be enabled or disabled. When enabled the relay will report its own internal errors to sentry.io to help us debug it. Lastly the relay will ask it should run authenticated with credentials or not. Currently we do not yet support authenticated mode against sentry.io. You now have a folder named .semaphore in your current working directory. To launch the server, run: semaphore run If you moved your config folder somewhere else, you can use the -c option: semaphore run - c . / my / custom / semaphore_folder / Setting up a Project Right now Relay is only really usable in \"simple proxy mode\" (without credentials), and as such calls the same exact endpoints on Sentry that an SDK would. That also means you have to configure each project individually in Relay. Create a new file in the form project_id.json : .semaphore / projects / ___PROJECT_ID___.json With the following content: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ], \"config\" : { \"allowedDomains\" : [ \"*\" ], \"piiConfig\" : {} } } The relay has to know all public keys (i.e. the secret part of the DSN) that will send events to it. DSNs unknown for this project will be rejected. Sending a Test Event Launch the server with semaphore run , and set up any SDK with the following DSN: http : //___PUBLIC_KEY___@127.0.0.1:3000/___PROJECT_ID___ As you can see we only changed the host and port of the DSN to point to your Relay setup. You should be able to use the SDK normally at this point. Events arrive at the Sentry instance that Relay is configured to use in .semaphore/config.yml . PII Stripping Now let's get to the entire point of this proxy setup: Stripping sensitive data. The easiest way to go about this is if you already have a raw JSON payload from some SDK. Go to our PII config editor Piinguin , and: Paste in a raw event Click on data you want eliminated Paste in other payloads and see if they look fine, go to step 2 if necessary. After iterating on the config, paste it back into the project config you created earlier: .semaphore / projects / ___PROJECT_ID___.json For example: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ], \"config\" : { \"allowedDomains\" : [ \"*\" ], \"piiConfig\" : { \"rules\" : { \"device_id\" : { \"type\" : \"pattern\" , \"pattern\" : \"d/[a-f0-9]{12}\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"freeform\" : [ \"device_id\" ] } } } }","title":"Introduction"},{"location":"#introduction","text":"The Sentry Relay (aka Semaphore) is a work in progress service that pushes some functionality from the Sentry SDKs as well as the Sentry server into a proxy process.","title":"Introduction"},{"location":"#getting-started","text":"The relay server is called \"semaphore\". Binaries can be downloaded from the GitHub releases page . After downloading place the binary somewhere on your PATH and make it executable. The config init command is provided to initialize the initial config. The config will be created the folder it's run from in a hidden .semaphore subdirectory: $ semaphore config init The wizard will ask a few questions: default vs custom config. In the default config the relay will connect to sentry.io for sending and will generally use the defaults. In the custom config a different upstream can be configured. Relay internal crash reporting can be enabled or disabled. When enabled the relay will report its own internal errors to sentry.io to help us debug it. Lastly the relay will ask it should run authenticated with credentials or not. Currently we do not yet support authenticated mode against sentry.io. You now have a folder named .semaphore in your current working directory. To launch the server, run: semaphore run If you moved your config folder somewhere else, you can use the -c option: semaphore run - c . / my / custom / semaphore_folder /","title":"Getting started"},{"location":"#setting-up-a-project","text":"Right now Relay is only really usable in \"simple proxy mode\" (without credentials), and as such calls the same exact endpoints on Sentry that an SDK would. That also means you have to configure each project individually in Relay. Create a new file in the form project_id.json : .semaphore / projects / ___PROJECT_ID___.json With the following content: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ], \"config\" : { \"allowedDomains\" : [ \"*\" ], \"piiConfig\" : {} } } The relay has to know all public keys (i.e. the secret part of the DSN) that will send events to it. DSNs unknown for this project will be rejected.","title":"Setting up a Project"},{"location":"#sending-a-test-event","text":"Launch the server with semaphore run , and set up any SDK with the following DSN: http : //___PUBLIC_KEY___@127.0.0.1:3000/___PROJECT_ID___ As you can see we only changed the host and port of the DSN to point to your Relay setup. You should be able to use the SDK normally at this point. Events arrive at the Sentry instance that Relay is configured to use in .semaphore/config.yml .","title":"Sending a Test Event"},{"location":"#pii-stripping","text":"Now let's get to the entire point of this proxy setup: Stripping sensitive data. The easiest way to go about this is if you already have a raw JSON payload from some SDK. Go to our PII config editor Piinguin , and: Paste in a raw event Click on data you want eliminated Paste in other payloads and see if they look fine, go to step 2 if necessary. After iterating on the config, paste it back into the project config you created earlier: .semaphore / projects / ___PROJECT_ID___.json For example: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ], \"config\" : { \"allowedDomains\" : [ \"*\" ], \"piiConfig\" : { \"rules\" : { \"device_id\" : { \"type\" : \"pattern\" , \"pattern\" : \"d/[a-f0-9]{12}\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"freeform\" : [ \"device_id\" ] } } } }","title":"PII Stripping"},{"location":"options/","text":"Configuration Options The base configuration for Relay lives in the file .semaphore/config.yml . All keys are snake_case . Relay General relay settings. relay.mode : string, default: managed Controls the basic communication and configuration mode for this relay. Allowed values are: managed (default) : Project configurations are managed by Sentry, unless they are statically overridden via the file system. This requires credentials to be set up and white listed in Sentry. static : Projects must be statically configured on the file system. If configured, PII stripping is also performed on those events. Events for unknown projects are automatically rejected. proxy : Relay acts as a proxy for all requests and events. It will not load project configurations from the upstream or perform PII stripping. All events are accepted unless overridden on the file system. For more information on providing or overriding project configurations on the file system, please refer to Project Configuration and PII Configuration . relay.upstream : string, default: https://sentry.io The upstream relay or sentry instance. Important : Relay does not check for cycles. Ensure this option is not set to an endpoint that will cause events to be cycled back here. relay.host : string, default: 127.0.0.1 The host the relay should bind to (network interface). Example: 0.0.0.0 relay.port : integer, default: 3000 The port to bind for the unencrypted relay HTTP server. Example: 3000 relay.tls_port : integer, optional Optional port to bind for the encrypted relay HTTPS server. Example: 3001 This is in addition to the port option: If you set up a HTTPS server at tls_port , the HTTP server at port still exists. relay.tls_identity_path : string, optional The filesystem path to the identity (DER-encoded PKCS12) to use for the HTTPS server. Example: relay_dev.pfx relay.tls_identity_password : string, optional Password for the PKCS12 archive in tls_identity_path . HTTP Set various network-related settings. http.timeout : integer, default: 5 Timeout for upstream requests in seconds. http.max_retry_interval : integer, default: 60 Maximum interval between failed request retries in seconds. Caching Fine-tune caching of project state. cache.project_expiry : integer, default: 300 (5 minutes) The cache timeout for project configurations in seconds. Irrelevant if you use the \"simple proxy mode\", where your project config is stored in a local file. cache.relay_expiry : integer, default: 3600 (1 hour) The cache timeout for downstream relay info (public keys) in seconds. cache.event_expiry : integer, default: 600 (10 minutes) The cache timeout for events (store) before dropping them. cache.miss_expiry : integer, default: 60 (1 minute) The cache timeout for non-existing entries. cache.batch_interval : integer, default: 100 (100 milliseconds) The buffer timeout for batched queries before sending them upstream in milliseconds . cache.file_interval : integer, default: 10 (10 seconds) Interval for watching local cache override files in seconds. cache.event_buffer_size : integer, default: 1000 The maximum number of events that are buffered in case of network issues or high rates of incoming events. Size Limits Controls various HTTP-related limits. All values are human-readable strings of a number and a human-readable unit, such as: 1KiB 1MB 1MiB 1MiB 1025B limits.max_concurrent_requests : integer, default: 100 The maximum number of concurrent connections to the upstream. limits.max_event_payload_size : string, default: 256KB The maximum payload size for events. limits.max_api_payload_size : string, default: 20MB The maximum payload size for general API requests. limits.max_api_file_upload_size : string, default: 40MB The maximum payload size for file uploads and chunks. limits.max_api_chunk_upload_size : string, default: 100MB The maximum payload size for chunks Logging logging.level : string, default: info The log level for the relay. One of: off error warn info debug trace logging.log_failed_payloads : boolean, default: false If set to true this emits log messages for failed event payloads. logging.format : string, default: auto Controls the log format. One of: auto : Auto detect (pretty for tty, simplified for other) pretty : With colors simplified : Simplified log output json : Dump out JSON lines logging.enable_backtraces : boolean, default: true When set to true, backtraces are forced on. Statsd Metrics metrics.statsd : string, optional If set to a host/port string then metrics will be reported to this statsd instance. metrics.prefix : string, default: sentry.relay The prefix that should be added to all metrics. Internal Error Reporting Configures error reporting for errors happening within Sentry. Disabled by default. sentry.enabled : boolean, default: false Whether to report internal errors to a separate DSN. false means no internal errors are sent (just logged). sentry.dsn : string, optional DSN to report internal relay failures to. It is not a good idea to set this to a value that will make the relay send errors to itself. Ideally this should just send errors to Sentry directly, not another relay.","title":"Configuration Options"},{"location":"options/#configuration-options","text":"The base configuration for Relay lives in the file .semaphore/config.yml . All keys are snake_case .","title":"Configuration Options"},{"location":"options/#relay","text":"General relay settings. relay.mode : string, default: managed Controls the basic communication and configuration mode for this relay. Allowed values are: managed (default) : Project configurations are managed by Sentry, unless they are statically overridden via the file system. This requires credentials to be set up and white listed in Sentry. static : Projects must be statically configured on the file system. If configured, PII stripping is also performed on those events. Events for unknown projects are automatically rejected. proxy : Relay acts as a proxy for all requests and events. It will not load project configurations from the upstream or perform PII stripping. All events are accepted unless overridden on the file system. For more information on providing or overriding project configurations on the file system, please refer to Project Configuration and PII Configuration . relay.upstream : string, default: https://sentry.io The upstream relay or sentry instance. Important : Relay does not check for cycles. Ensure this option is not set to an endpoint that will cause events to be cycled back here. relay.host : string, default: 127.0.0.1 The host the relay should bind to (network interface). Example: 0.0.0.0 relay.port : integer, default: 3000 The port to bind for the unencrypted relay HTTP server. Example: 3000 relay.tls_port : integer, optional Optional port to bind for the encrypted relay HTTPS server. Example: 3001 This is in addition to the port option: If you set up a HTTPS server at tls_port , the HTTP server at port still exists. relay.tls_identity_path : string, optional The filesystem path to the identity (DER-encoded PKCS12) to use for the HTTPS server. Example: relay_dev.pfx relay.tls_identity_password : string, optional Password for the PKCS12 archive in tls_identity_path .","title":"Relay"},{"location":"options/#http","text":"Set various network-related settings. http.timeout : integer, default: 5 Timeout for upstream requests in seconds. http.max_retry_interval : integer, default: 60 Maximum interval between failed request retries in seconds.","title":"HTTP"},{"location":"options/#caching","text":"Fine-tune caching of project state. cache.project_expiry : integer, default: 300 (5 minutes) The cache timeout for project configurations in seconds. Irrelevant if you use the \"simple proxy mode\", where your project config is stored in a local file. cache.relay_expiry : integer, default: 3600 (1 hour) The cache timeout for downstream relay info (public keys) in seconds. cache.event_expiry : integer, default: 600 (10 minutes) The cache timeout for events (store) before dropping them. cache.miss_expiry : integer, default: 60 (1 minute) The cache timeout for non-existing entries. cache.batch_interval : integer, default: 100 (100 milliseconds) The buffer timeout for batched queries before sending them upstream in milliseconds . cache.file_interval : integer, default: 10 (10 seconds) Interval for watching local cache override files in seconds. cache.event_buffer_size : integer, default: 1000 The maximum number of events that are buffered in case of network issues or high rates of incoming events.","title":"Caching"},{"location":"options/#size-limits","text":"Controls various HTTP-related limits. All values are human-readable strings of a number and a human-readable unit, such as: 1KiB 1MB 1MiB 1MiB 1025B limits.max_concurrent_requests : integer, default: 100 The maximum number of concurrent connections to the upstream. limits.max_event_payload_size : string, default: 256KB The maximum payload size for events. limits.max_api_payload_size : string, default: 20MB The maximum payload size for general API requests. limits.max_api_file_upload_size : string, default: 40MB The maximum payload size for file uploads and chunks. limits.max_api_chunk_upload_size : string, default: 100MB The maximum payload size for chunks","title":"Size Limits"},{"location":"options/#logging","text":"logging.level : string, default: info The log level for the relay. One of: off error warn info debug trace logging.log_failed_payloads : boolean, default: false If set to true this emits log messages for failed event payloads. logging.format : string, default: auto Controls the log format. One of: auto : Auto detect (pretty for tty, simplified for other) pretty : With colors simplified : Simplified log output json : Dump out JSON lines logging.enable_backtraces : boolean, default: true When set to true, backtraces are forced on.","title":"Logging"},{"location":"options/#statsd-metrics","text":"metrics.statsd : string, optional If set to a host/port string then metrics will be reported to this statsd instance. metrics.prefix : string, default: sentry.relay The prefix that should be added to all metrics.","title":"Statsd Metrics"},{"location":"options/#internal-error-reporting","text":"Configures error reporting for errors happening within Sentry. Disabled by default. sentry.enabled : boolean, default: false Whether to report internal errors to a separate DSN. false means no internal errors are sent (just logged). sentry.dsn : string, optional DSN to report internal relay failures to. It is not a good idea to set this to a value that will make the relay send errors to itself. Ideally this should just send errors to Sentry directly, not another relay.","title":"Internal Error Reporting"},{"location":"project-config/","text":"Project Configuration A Relay will generally not accept or forward events where it doesn't know the project. See Getting Started for a short introduction to project configs. This page enumerates all possible options. The configuration for a project with the ID 123 lives in the file .semaphore/projects/123.json . You can get the project ID from the path section of a DSN. Basic Options disabled : boolean, default: false Whether the project is disabled. If set to true , the Relay will drop all events sent to this project. publicKeys : array of objects, required A map enumerating known public keys (the public key in a DSN) and whether events using that key should be accepted. Example: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ] } {:.config-key} Config config.allowedDomains : array of origins (strings), default: [\"*\"] Configure origin URLs which Sentry should accept events from. This is corresponds to the \"Allowed Domains\" setting in the Sentry UI. Note that an empty array will reject all origins. Use the default [\"*\"] to allow all origins. config.piiConfig : See PII Configuration .","title":"Project Configuration"},{"location":"project-config/#project-configuration","text":"A Relay will generally not accept or forward events where it doesn't know the project. See Getting Started for a short introduction to project configs. This page enumerates all possible options. The configuration for a project with the ID 123 lives in the file .semaphore/projects/123.json . You can get the project ID from the path section of a DSN.","title":"Project Configuration"},{"location":"project-config/#basic-options","text":"disabled : boolean, default: false Whether the project is disabled. If set to true , the Relay will drop all events sent to this project. publicKeys : array of objects, required A map enumerating known public keys (the public key in a DSN) and whether events using that key should be accepted. Example: { \"publicKeys\" : [ { \"publicKey\" : \"___PUBLIC_KEY___\" , \"isEnabled\" : true } ] } {:.config-key}","title":"Basic Options"},{"location":"project-config/#config","text":"config.allowedDomains : array of origins (strings), default: [\"*\"] Configure origin URLs which Sentry should accept events from. This is corresponds to the \"Allowed Domains\" setting in the Sentry UI. Note that an empty array will reject all origins. Use the default [\"*\"] to allow all origins. config.piiConfig : See PII Configuration .","title":"Config"},{"location":"architecture/","text":"Architecture This subsection contains internal docs that are useful for development and operation of Relay","title":"Architecture"},{"location":"architecture/#architecture","text":"This subsection contains internal docs that are useful for development and operation of Relay","title":"Architecture"},{"location":"architecture/actors/","text":"Actors This document describes how Semaphore works through the perspective of the system actors and the messages exchanged by them. TODO Short description about infrastructure (i.e. actix-web, actix, tokio, futures), note that we are using the old style Future trait, and actix 0.7.x . controller.rs events.rs The events.rs module contains functionality related to the processing of events. Raw events being sent to the system are first processed, and then sent to Sentry for saving. The module contains to actors: EventManager : an actor that receives all events, checks if the event should be processed (i.e. it is not filtered) and orchestrates all data needed for event processing. If the event should be processed it is passed to the EventProcessor . There is one EventManager in the system. EventProcessor : an CPU bound actor that receives all necessary project configuration and does the heavy lifting of processing the message (normalisation, PII stripping ...). There are multiple EventProcessor actors running in a thread pool. EventManager The EventManager is an actor running in the main system arbiter. The system arbiter is a asynchronous arbiter created when the Semaphore starts. The upshot of this is that all processing done by the event manager should be extremly quick. It is ok to wait on I/O but no significant processing may be done in the EventManager . Once the EventManager had obtained the project state and had decided that the event should be processed the EventManager passes the event to the EventProcessor actors. TODO document all handlers EventProcessor The EventProcessor is an actor running in a SyncArbiter. There are multiple instances of the EventProcessor actor (roughly 1 per thread) ( TODO RaduW check that I'm not talking nonsense here.). The EventProcessor does the heavy lifting of event processing. The event processing does only synchronous work ( all the IO is handleed in the EventManager and all the needed state is passed to the EventProcessor ) Since all the work done by the event processor is the synchronous processing of an event there is only one type of message accepted by the EventProcessor actor: ProcessEvent The ProcessEvent handler prepares the event for ingestion. It normalizes the event, symbolicates its stack-trace and strips sensitive information (PPI stripping). TODO finish here keys.rs outcome.rs server.rs store.rs upstream.rs project.rs The project.rs module contains functionality related to the project state. Sentry events belong to projects and projects belong to organizations (Semaphore doesn't care at the moment about organizations). Projects serve to group the events (so that each Sentry customer can only see and deal with his/hers own events) and prescribe how messages are to be processed ( what should be filtered, which data inside the event should be anonymised (i.e PPI stripping), etc). All activities around obtaining, caching and refreshing the project state is handled in this module. The module contains two actors: Project : an actor that holds project data ProjectCache : an actor that holds references to Project actors. From a high level perspective one obtains a Project from the ProjectCache and then uses the Project in order to get 'project specific' information. Project The Project actor is responsible with decisions about how an event for a particular project should be handled. The project runs in an async arbiter ( I think it actully runs in the SystemArbiter TODO check ir that is true, or if it runs in another async arbiter). The system constructs an actor for each project used. The Project actor runs in an async context. The Project actor responds to the following message types: GetProjectId The GetProjectId handler returns the project_id (trivial functionality). GetProjectState The handler retuns the ProjectState , either directly (if it has a recent cached version) or by asking the ProjectCache to fetch it on its behalf. Pseudo Code if 'we have an up-to-date project state' return self . project_state if not self . receiver : # we don't have an already active request for the project state channel , receiver = 'create a channel and save its receiver' async : # ops that run at some time in the future project_state = await ProjectCache . send ( FetchProjectState ) channel . sender . push ( project_state ) # put the state on the channel when available channel = None # clenaup after we have pushed the ProjectState return future ( receiver ) # a Future that resolves with the ProjectState when available GetEventAction The handler answers the question: How should an event be handled? An event may be handled in one of three ways specified by the EventAction enum. pub enum EventAction { /// The event should be discarded. Discard ( DiscardReason ), /// The event should be discarded and the client should back off for some time. RetryAfter ( RateLimit ), /// The event should be processed and sent to upstream. Accept , } Pseudo Code if 'we have a cached rate limit value for the current event key' : return RetryAfter # the event is discarded without further processing if 'we have a recent project state' # Ask the ProjectState what to do with the msg return projectState . get_event_action ( event ) # No recent ProjectState, fetch it from the ProjectCache actor async : # ops that run at soem time in the future projectState = await ProjectCache . send ( FetchProjectState ) # return a future that will resolve when the project state becomes avaialble return future ( projectState . get_event_action ( event )) RateLimit The handler is used to update chached versions of project RateLimits. Pseudo Code if rate_limit . type not in self . rate_limit_cache self . rate_limit_cache . insert ( rate_limit ) # insert a new rate limit else if \"rate_limit expires sooner than the old cached value\" : self . rate_limit_cache . insert ( rate_limit ) # update old rate limit ProjectCache The ProjectCache actor is responsible for providing Project actors. The ProjectCache runs in an asynchronous context. There is only one instance of the ProjectCache in the system. The project runs in an async arbiter ( I think it actully runs in the SystemArbiter TODO check ir that is true, or if it runs in another async arbiter). The ProjectCache actor responds to the following messages: GetProject Retuns a Project actor for the required project. Pseudo Code if project_id not in self . project_cache : project = Project ( project_id ) project . start () self . project_cache [ project_id ] = project return self . project_cache [ porject_id ] FetchProjectState Registers the intention of a project to retrieve the project state. The FetchProjectState functionality logicaly belongs to the Project actor but it is handled by the ProjectCache in order to batch requests for the project state from multiple Project actors. A Project registers its desire to obtain its project state with the ProjectCache by sending the FetchProjectState and the ProjectCache batches all requests during a batching period and emits one request to the upstream (another semaphore or the Sentry server) for all required project states. The handler checks if there is already a scheduled time for fetching project states and, if not, it schedules a delayed function that will do the actual fetching for all projects that registered their desire for getting the project state (see function: ProjectCache::fetch_states ). TODO: Add pseudo code UpdateLocalStates Updates project states for project states comming from a local file configuration.(TODO check if that is correct RaduW. 10.Oct.2019) This message is emmited periodically by a thread that loops and periodically checks the local file system for configuration changes (see function: poll_local_states ). Store endpoint request processing graph LR Store>/api/#PRJ#/store<br/>store.storeEvent] Upstream(UpstreamRelay) EvMan(EventManager) Proj(Project) EvProc[\"EventProcessor <br/>(sync)\"] Cache(ProjectCache) StoreF(StoreForwarder) Store-->|GetEventAction|Proj Store-->|GetProject|Cache Store-->|QueueEvent|EvMan Proj-->|FetchProjectState|Cache EvMan-->|GetProjectState|Proj EvMan-->|GetEventAction|Proj EvMan-->|RateLimit|Proj EvMan-->|HandleEvent|EvMan EvMan-->|GetProjectId|Proj EvMan-->|ProcessEvent|EvProc EvMan-->|StoreEvent|StoreF EvMan-->|SendRequest|Upstream Cache-->|UpdateLocalStates|Cache Event ingestion Title : Event Ingestion store_event --> ProjectCache : GetProject ProjectCache --> store_event : Project store_event --> Project : GetEventAction Project --> store_event : EventAction store_event --> EventManager : QueueEvent EventManager --> store_event : event_id EventManager --> EventManager : HandleEvent EventManager --> Project : GetProjectId Project --> EventManager : ProjectId EventManager --> Project : GetEventAction Project --> EventManager : EventAction EventManager --> Project : GetProjectState Project --> EventManager : ProjectState EventManager --> EventProcessor : ProcessEvent EventProcessor --> EventManager : ProcessEventResponse Server bootstrap main . main -> cli . execute : cli . execute -> cli . execute : make_app cli . execute -> server . lib . run : server . lib . run -> Controller : run Controller -> server . actors . Server : start server . actors . Server -> server . service : start server . service -> ServiceState : start ServiceState start ServiceState -> UpstreamRelay : start ( config ) ServiceState -> EventManager : start ( config , upstream_realy ) ServiceState -> KeyCache : start ( config , upstream_relay ) ServiceState -> ProjectCache : start ( config , upstream_relay )","title":"Actors"},{"location":"architecture/actors/#actors","text":"This document describes how Semaphore works through the perspective of the system actors and the messages exchanged by them. TODO Short description about infrastructure (i.e. actix-web, actix, tokio, futures), note that we are using the old style Future trait, and actix 0.7.x .","title":"Actors"},{"location":"architecture/actors/#controllerrs","text":"","title":"controller.rs"},{"location":"architecture/actors/#eventsrs","text":"The events.rs module contains functionality related to the processing of events. Raw events being sent to the system are first processed, and then sent to Sentry for saving. The module contains to actors: EventManager : an actor that receives all events, checks if the event should be processed (i.e. it is not filtered) and orchestrates all data needed for event processing. If the event should be processed it is passed to the EventProcessor . There is one EventManager in the system. EventProcessor : an CPU bound actor that receives all necessary project configuration and does the heavy lifting of processing the message (normalisation, PII stripping ...). There are multiple EventProcessor actors running in a thread pool.","title":"events.rs"},{"location":"architecture/actors/#eventmanager","text":"The EventManager is an actor running in the main system arbiter. The system arbiter is a asynchronous arbiter created when the Semaphore starts. The upshot of this is that all processing done by the event manager should be extremly quick. It is ok to wait on I/O but no significant processing may be done in the EventManager . Once the EventManager had obtained the project state and had decided that the event should be processed the EventManager passes the event to the EventProcessor actors. TODO document all handlers","title":"EventManager"},{"location":"architecture/actors/#eventprocessor","text":"The EventProcessor is an actor running in a SyncArbiter. There are multiple instances of the EventProcessor actor (roughly 1 per thread) ( TODO RaduW check that I'm not talking nonsense here.). The EventProcessor does the heavy lifting of event processing. The event processing does only synchronous work ( all the IO is handleed in the EventManager and all the needed state is passed to the EventProcessor ) Since all the work done by the event processor is the synchronous processing of an event there is only one type of message accepted by the EventProcessor actor:","title":"EventProcessor"},{"location":"architecture/actors/#processevent","text":"The ProcessEvent handler prepares the event for ingestion. It normalizes the event, symbolicates its stack-trace and strips sensitive information (PPI stripping). TODO finish here","title":"ProcessEvent"},{"location":"architecture/actors/#keysrs","text":"","title":"keys.rs"},{"location":"architecture/actors/#outcomers","text":"","title":"outcome.rs"},{"location":"architecture/actors/#serverrs","text":"","title":"server.rs"},{"location":"architecture/actors/#storers","text":"","title":"store.rs"},{"location":"architecture/actors/#upstreamrs","text":"","title":"upstream.rs"},{"location":"architecture/actors/#projectrs","text":"The project.rs module contains functionality related to the project state. Sentry events belong to projects and projects belong to organizations (Semaphore doesn't care at the moment about organizations). Projects serve to group the events (so that each Sentry customer can only see and deal with his/hers own events) and prescribe how messages are to be processed ( what should be filtered, which data inside the event should be anonymised (i.e PPI stripping), etc). All activities around obtaining, caching and refreshing the project state is handled in this module. The module contains two actors: Project : an actor that holds project data ProjectCache : an actor that holds references to Project actors. From a high level perspective one obtains a Project from the ProjectCache and then uses the Project in order to get 'project specific' information.","title":"project.rs"},{"location":"architecture/actors/#project","text":"The Project actor is responsible with decisions about how an event for a particular project should be handled. The project runs in an async arbiter ( I think it actully runs in the SystemArbiter TODO check ir that is true, or if it runs in another async arbiter). The system constructs an actor for each project used. The Project actor runs in an async context. The Project actor responds to the following message types:","title":"Project"},{"location":"architecture/actors/#getprojectid","text":"The GetProjectId handler returns the project_id (trivial functionality).","title":"GetProjectId"},{"location":"architecture/actors/#getprojectstate","text":"The handler retuns the ProjectState , either directly (if it has a recent cached version) or by asking the ProjectCache to fetch it on its behalf.","title":"GetProjectState"},{"location":"architecture/actors/#pseudo-code","text":"if 'we have an up-to-date project state' return self . project_state if not self . receiver : # we don't have an already active request for the project state channel , receiver = 'create a channel and save its receiver' async : # ops that run at some time in the future project_state = await ProjectCache . send ( FetchProjectState ) channel . sender . push ( project_state ) # put the state on the channel when available channel = None # clenaup after we have pushed the ProjectState return future ( receiver ) # a Future that resolves with the ProjectState when available","title":"Pseudo Code"},{"location":"architecture/actors/#geteventaction","text":"The handler answers the question: How should an event be handled? An event may be handled in one of three ways specified by the EventAction enum. pub enum EventAction { /// The event should be discarded. Discard ( DiscardReason ), /// The event should be discarded and the client should back off for some time. RetryAfter ( RateLimit ), /// The event should be processed and sent to upstream. Accept , }","title":"GetEventAction"},{"location":"architecture/actors/#pseudo-code_1","text":"if 'we have a cached rate limit value for the current event key' : return RetryAfter # the event is discarded without further processing if 'we have a recent project state' # Ask the ProjectState what to do with the msg return projectState . get_event_action ( event ) # No recent ProjectState, fetch it from the ProjectCache actor async : # ops that run at soem time in the future projectState = await ProjectCache . send ( FetchProjectState ) # return a future that will resolve when the project state becomes avaialble return future ( projectState . get_event_action ( event ))","title":"Pseudo Code"},{"location":"architecture/actors/#ratelimit","text":"The handler is used to update chached versions of project RateLimits.","title":"RateLimit"},{"location":"architecture/actors/#pseudo-code_2","text":"if rate_limit . type not in self . rate_limit_cache self . rate_limit_cache . insert ( rate_limit ) # insert a new rate limit else if \"rate_limit expires sooner than the old cached value\" : self . rate_limit_cache . insert ( rate_limit ) # update old rate limit","title":"Pseudo Code"},{"location":"architecture/actors/#projectcache","text":"The ProjectCache actor is responsible for providing Project actors. The ProjectCache runs in an asynchronous context. There is only one instance of the ProjectCache in the system. The project runs in an async arbiter ( I think it actully runs in the SystemArbiter TODO check ir that is true, or if it runs in another async arbiter). The ProjectCache actor responds to the following messages:","title":"ProjectCache"},{"location":"architecture/actors/#getproject","text":"Retuns a Project actor for the required project.","title":"GetProject"},{"location":"architecture/actors/#pseudo-code_3","text":"if project_id not in self . project_cache : project = Project ( project_id ) project . start () self . project_cache [ project_id ] = project return self . project_cache [ porject_id ]","title":"Pseudo Code"},{"location":"architecture/actors/#fetchprojectstate","text":"Registers the intention of a project to retrieve the project state. The FetchProjectState functionality logicaly belongs to the Project actor but it is handled by the ProjectCache in order to batch requests for the project state from multiple Project actors. A Project registers its desire to obtain its project state with the ProjectCache by sending the FetchProjectState and the ProjectCache batches all requests during a batching period and emits one request to the upstream (another semaphore or the Sentry server) for all required project states. The handler checks if there is already a scheduled time for fetching project states and, if not, it schedules a delayed function that will do the actual fetching for all projects that registered their desire for getting the project state (see function: ProjectCache::fetch_states ). TODO: Add pseudo code","title":"FetchProjectState"},{"location":"architecture/actors/#updatelocalstates","text":"Updates project states for project states comming from a local file configuration.(TODO check if that is correct RaduW. 10.Oct.2019) This message is emmited periodically by a thread that loops and periodically checks the local file system for configuration changes (see function: poll_local_states ). Store endpoint request processing graph LR Store>/api/#PRJ#/store<br/>store.storeEvent] Upstream(UpstreamRelay) EvMan(EventManager) Proj(Project) EvProc[\"EventProcessor <br/>(sync)\"] Cache(ProjectCache) StoreF(StoreForwarder) Store-->|GetEventAction|Proj Store-->|GetProject|Cache Store-->|QueueEvent|EvMan Proj-->|FetchProjectState|Cache EvMan-->|GetProjectState|Proj EvMan-->|GetEventAction|Proj EvMan-->|RateLimit|Proj EvMan-->|HandleEvent|EvMan EvMan-->|GetProjectId|Proj EvMan-->|ProcessEvent|EvProc EvMan-->|StoreEvent|StoreF EvMan-->|SendRequest|Upstream Cache-->|UpdateLocalStates|Cache Event ingestion Title : Event Ingestion store_event --> ProjectCache : GetProject ProjectCache --> store_event : Project store_event --> Project : GetEventAction Project --> store_event : EventAction store_event --> EventManager : QueueEvent EventManager --> store_event : event_id EventManager --> EventManager : HandleEvent EventManager --> Project : GetProjectId Project --> EventManager : ProjectId EventManager --> Project : GetEventAction Project --> EventManager : EventAction EventManager --> Project : GetProjectState Project --> EventManager : ProjectState EventManager --> EventProcessor : ProcessEvent EventProcessor --> EventManager : ProcessEventResponse Server bootstrap main . main -> cli . execute : cli . execute -> cli . execute : make_app cli . execute -> server . lib . run : server . lib . run -> Controller : run Controller -> server . actors . Server : start server . actors . Server -> server . service : start server . service -> ServiceState : start ServiceState start ServiceState -> UpstreamRelay : start ( config ) ServiceState -> EventManager : start ( config , upstream_realy ) ServiceState -> KeyCache : start ( config , upstream_relay ) ServiceState -> ProjectCache : start ( config , upstream_relay )","title":"UpdateLocalStates"},{"location":"architecture/ingest-event-path/","text":"Path of an Event through Relay Processing enabled vs not? Relay can run as part of a Sentry installation, such as within sentry.io 's infrastructure, or next to the application as a forwarding proxy. A lot of steps described here are skipped or run in a limited form when Relay is not running in processing mode: Event normalization does different (less) things. In static mode, project config is read from disk instead of fetched from an HTTP endpoint. In proxy mode, project config is just filled out with defaults. Events are forwarded to an HTTP endpoint instead of being written to Kafka. Rate limits are not calculated using Redis, instead Relay just honors 429s from previously mentioned endpoint. Filters are not applied at all. Inside the endpoint When an SDK hits /api/X/store on Relay, the code in server/src/endpoints/store.rs is called before returning a HTTP response. That code looks into an in-memory cache to answer basic information about a project such as: Does it exist? Is it suspended/disabled? Is it rate limited right now? If so, which key is rate limited? Which DSNs are valid for this project? Some of the data for this cache is coming from the projectconfigs endpoint . It is refreshed every couple of minutes, depending on configuration. If the cache is fresh, we may return a 429 for rate limits or a 4xx for invalid auth information. That cache might be empty or stale. If that is the case, Relay does not actually attempt to populate it at this stage. It just returns a 200 even though the event might be dropped later. This implies: The first store request that runs into a rate limit doesn't actually result in a 429 , but a subsequent request will (because by that time the project cache will have been updated). A store request for a non-existent project may result in a 200 , but subsequent ones will not. A store request with wrong auth information may result in a 200 , but subsequent ones will not. Filters are also not applied at this stage, so a filtered event will always result in a 200 . This matches the Python behavior since a while now . These examples assume that a project receives one event at a time. In practice one may observe that a highly concurrent burst of store requests for a single project results in 200 OK s only. However, a multi-second flood of incoming events should quickly result in eventually consistent and correct status codes. The response is completed at this point. All expensive work (such as talking to external services) is deferred into a background task. Except for responding to the HTTP request, there's no I/O done in the endpoint in any form. We didn't even hit Redis to calculate rate limits. The fast response times are supposed to benefit application environments which cannot send HTTP requests fully asynchronously (PHP and certain serverless platorms), at the cost of status codes being inaccurate. Note that we still emit accurate outcomes into Kafka if configured to do so. The background task The HTTP response is out, with a status code that is just a best-effort guess at what the actual outcome of the event is going to be. The rest of what used to happen synchronously in the Python StoreView is done asynchronously, but still in the same process. So, now to the real work: Project config is fetched. If the project cache is stale or missing, we fetch it. We may wait a couple milliseconds here to be able to batch multiple project config fetches into the same HTTP request to not overload Sentry too much. At this stage Relay may drop the event because it realized that the DSN was invalid or the project didn't even exist. The next incoming event will get a proper 4xx status code. The event is parsed. In the endpoint we only did decompression, a basic JSON syntax check, and extraction of the event ID to be able to return it as part of the response. Now we create an Event struct, which conceptually is the equivalent to parsing it into a Python dictionary: We allocate more memory. The event is datascrubbed. We have a PII config (new format) and a datascrubbing config (old format, converted to new format on the fly) as part of the project config fetched from Sentry. The event is normalized. Event normalization is probably the most CPU-intensive task running in Relay. It discards invalid data, moves data from deprecated fields to newer fields and generally just does schema validation. Filters (\"inbound filters\") are applied. Event may be discarded because of IP addresses, patterns on the error message or known web crawlers. Exact rate limits (\"quotas\") are applied. is_rate_limited.lua is executed on Redis. The input parameters for is_rate_limited.lua (\"quota objects\") are part of the project config. See this pull request for an explanation of what quota objects are. The event may be discarded here. If so, we write the rate limit info (reason and expiration timestamp) into the in-memory project cache so that the next store request returns a 429 in the endpoint and doesn't hit Redis at all. This contraption has the advantage that suspended or permanently rate-limited projects are very cheap to handle, and do not involve external services (ignoring the polling of the project config every couple of minutes). Event is written to Kafka. Note: If we discard an event at any point, an outcome is written to Kafka if Relay is configured to do so. The outcomes consumer Outcomes are small messages in Kafka that contain an event ID and information about whether that event was rejected, and if so, why. The outcomes consumer is mostly responsible for updating (user-visible) counters in Sentry (buffers/counters and tsdb, which are two separate systems). The ingest consumer The ingest consumer reads accepted events from Kafka, and also updates some stats. Some of those stats are billing-relevant. Its main purpose is to do what insert_data_to_database in Python store did: Call preprocess_event , after which comes sourcemap processing, native symbolication, grouping, snuba and all that other stuff that is of no concern to Relay.","title":"Path of an Event through Relay"},{"location":"architecture/ingest-event-path/#path-of-an-event-through-relay","text":"","title":"Path of an Event through Relay"},{"location":"architecture/ingest-event-path/#processing-enabled-vs-not","text":"Relay can run as part of a Sentry installation, such as within sentry.io 's infrastructure, or next to the application as a forwarding proxy. A lot of steps described here are skipped or run in a limited form when Relay is not running in processing mode: Event normalization does different (less) things. In static mode, project config is read from disk instead of fetched from an HTTP endpoint. In proxy mode, project config is just filled out with defaults. Events are forwarded to an HTTP endpoint instead of being written to Kafka. Rate limits are not calculated using Redis, instead Relay just honors 429s from previously mentioned endpoint. Filters are not applied at all.","title":"Processing enabled vs not?"},{"location":"architecture/ingest-event-path/#inside-the-endpoint","text":"When an SDK hits /api/X/store on Relay, the code in server/src/endpoints/store.rs is called before returning a HTTP response. That code looks into an in-memory cache to answer basic information about a project such as: Does it exist? Is it suspended/disabled? Is it rate limited right now? If so, which key is rate limited? Which DSNs are valid for this project? Some of the data for this cache is coming from the projectconfigs endpoint . It is refreshed every couple of minutes, depending on configuration. If the cache is fresh, we may return a 429 for rate limits or a 4xx for invalid auth information. That cache might be empty or stale. If that is the case, Relay does not actually attempt to populate it at this stage. It just returns a 200 even though the event might be dropped later. This implies: The first store request that runs into a rate limit doesn't actually result in a 429 , but a subsequent request will (because by that time the project cache will have been updated). A store request for a non-existent project may result in a 200 , but subsequent ones will not. A store request with wrong auth information may result in a 200 , but subsequent ones will not. Filters are also not applied at this stage, so a filtered event will always result in a 200 . This matches the Python behavior since a while now . These examples assume that a project receives one event at a time. In practice one may observe that a highly concurrent burst of store requests for a single project results in 200 OK s only. However, a multi-second flood of incoming events should quickly result in eventually consistent and correct status codes. The response is completed at this point. All expensive work (such as talking to external services) is deferred into a background task. Except for responding to the HTTP request, there's no I/O done in the endpoint in any form. We didn't even hit Redis to calculate rate limits. The fast response times are supposed to benefit application environments which cannot send HTTP requests fully asynchronously (PHP and certain serverless platorms), at the cost of status codes being inaccurate. Note that we still emit accurate outcomes into Kafka if configured to do so.","title":"Inside the endpoint"},{"location":"architecture/ingest-event-path/#the-background-task","text":"The HTTP response is out, with a status code that is just a best-effort guess at what the actual outcome of the event is going to be. The rest of what used to happen synchronously in the Python StoreView is done asynchronously, but still in the same process. So, now to the real work: Project config is fetched. If the project cache is stale or missing, we fetch it. We may wait a couple milliseconds here to be able to batch multiple project config fetches into the same HTTP request to not overload Sentry too much. At this stage Relay may drop the event because it realized that the DSN was invalid or the project didn't even exist. The next incoming event will get a proper 4xx status code. The event is parsed. In the endpoint we only did decompression, a basic JSON syntax check, and extraction of the event ID to be able to return it as part of the response. Now we create an Event struct, which conceptually is the equivalent to parsing it into a Python dictionary: We allocate more memory. The event is datascrubbed. We have a PII config (new format) and a datascrubbing config (old format, converted to new format on the fly) as part of the project config fetched from Sentry. The event is normalized. Event normalization is probably the most CPU-intensive task running in Relay. It discards invalid data, moves data from deprecated fields to newer fields and generally just does schema validation. Filters (\"inbound filters\") are applied. Event may be discarded because of IP addresses, patterns on the error message or known web crawlers. Exact rate limits (\"quotas\") are applied. is_rate_limited.lua is executed on Redis. The input parameters for is_rate_limited.lua (\"quota objects\") are part of the project config. See this pull request for an explanation of what quota objects are. The event may be discarded here. If so, we write the rate limit info (reason and expiration timestamp) into the in-memory project cache so that the next store request returns a 429 in the endpoint and doesn't hit Redis at all. This contraption has the advantage that suspended or permanently rate-limited projects are very cheap to handle, and do not involve external services (ignoring the polling of the project config every couple of minutes). Event is written to Kafka. Note: If we discard an event at any point, an outcome is written to Kafka if Relay is configured to do so.","title":"The background task"},{"location":"architecture/ingest-event-path/#the-outcomes-consumer","text":"Outcomes are small messages in Kafka that contain an event ID and information about whether that event was rejected, and if so, why. The outcomes consumer is mostly responsible for updating (user-visible) counters in Sentry (buffers/counters and tsdb, which are two separate systems).","title":"The outcomes consumer"},{"location":"architecture/ingest-event-path/#the-ingest-consumer","text":"The ingest consumer reads accepted events from Kafka, and also updates some stats. Some of those stats are billing-relevant. Its main purpose is to do what insert_data_to_database in Python store did: Call preprocess_event , after which comes sourcemap processing, native symbolication, grouping, snuba and all that other stuff that is of no concern to Relay.","title":"The ingest consumer"},{"location":"pii-config/","text":"PII Configuration The following document explores the syntax and semantics of PII configs for Relay. To get started with PII configs, it's recommended to use Piinguin and refer back to this document when needed. A basic example Say you have an exception message which, unfortunately, contains IP addresses which are not supposed to be there. You'd write: { \"applications\" : { \"text\" : [ \"@ip:replace\" ] } } It reads as \"apply rule @ip:replace to all text fields. Rules The following rules exist by default: @ip:replace and @ip:hash for pattern-matching IP addresses @imei:replace and @imei:hash for pattern-matching IMEIs @mac:replace , @mac:mask and @mac:hash for pattern-matching MAC addresses @email:mask , @email:replace and @email:hash for pattern-matching email addresses @creditcard:mask , @creditcard:replace and @creditcard:hash for pattern-matching creditcard numbers @userpath:replace and @userpath:hash for pattern-matching local paths (e.g. C:/Users/foo/ ) @password:remove for pattern matching passwords. In this case we're pattern matching against the field's key, whether it contains password , credentials or similar strings. Writing your own rules Rules generally consist of two parts: Rule types describe what to match. See PII Rule Types for an exhaustive list. Rule redaction methods describe what to do with the match. See PII Rule Redaction Methods for a list. Each page comes with examples. Try those examples out by pasting them into the \"PII config\" column of Piinguin and clicking on fields to get suggestions.","title":"PII Configuration"},{"location":"pii-config/#pii-configuration","text":"The following document explores the syntax and semantics of PII configs for Relay. To get started with PII configs, it's recommended to use Piinguin and refer back to this document when needed.","title":"PII Configuration"},{"location":"pii-config/#a-basic-example","text":"Say you have an exception message which, unfortunately, contains IP addresses which are not supposed to be there. You'd write: { \"applications\" : { \"text\" : [ \"@ip:replace\" ] } } It reads as \"apply rule @ip:replace to all text fields.","title":"A basic example"},{"location":"pii-config/#rules","text":"The following rules exist by default: @ip:replace and @ip:hash for pattern-matching IP addresses @imei:replace and @imei:hash for pattern-matching IMEIs @mac:replace , @mac:mask and @mac:hash for pattern-matching MAC addresses @email:mask , @email:replace and @email:hash for pattern-matching email addresses @creditcard:mask , @creditcard:replace and @creditcard:hash for pattern-matching creditcard numbers @userpath:replace and @userpath:hash for pattern-matching local paths (e.g. C:/Users/foo/ ) @password:remove for pattern matching passwords. In this case we're pattern matching against the field's key, whether it contains password , credentials or similar strings.","title":"Rules"},{"location":"pii-config/#writing-your-own-rules","text":"Rules generally consist of two parts: Rule types describe what to match. See PII Rule Types for an exhaustive list. Rule redaction methods describe what to do with the match. See PII Rule Redaction Methods for a list. Each page comes with examples. Try those examples out by pasting them into the \"PII config\" column of Piinguin and clicking on fields to get suggestions.","title":"Writing your own rules"},{"location":"pii-config/methods/","text":"PII Rule Redaction Methods remove Remove the entire field. Relay may choose to either set it to null or to remove it entirely. { \"rules\" : { \"remove_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"text\" : [ \"remove_ip\" ] } } replace Replace the key with a static string. { \"rules\" : { \"replace_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"replace\" , \"text\" : \"[censored]\" } } }, \"applications\" : { \"text\" : [ \"replace_ip\" ] } } mask Replace every character of the matched string with a \"masking\" char. Compared to replace this preserves the length of the original string. { \"rules\" : { \"mask_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"mask\" , \"mask_char\" : \"0\" , // The character used for masking. Optional, default \"*\" \"chars_to_ignore\" : \".\" , // Which characters to ignore. Optional, default empty \"range\" : [ 0 , - 1 ] // Which range of the string to replace. Optional, defaults to full range. Negative indices count from the matches' end. } } }, \"applications\" : { \"text\" : [ \"mask_ip\" ] } } hash Replace the string with a hashed version of itself. Equal strings will produce the same hash, so if you, for example, decide to hash the user ID instead of replacing or removing it, you will still have an accurate count of users affected. { \"rules\" : { \"hash_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"hash\" , \"algorithm\" : \"HMAC-SHA1\" , // One of \"HMAC-SHA1\", \"HMAC-SHA256\", \"HMAC-SHA512\" \"key\" : \"myOverriddenKey\" // A key to salt the hash with. Defaults to the default key set in \"vars\" } } }, \"vars\" : { \"hashKey\" : \"myDefaultKey\" // The default key to use } \"applications\" : { \"text\" : [ \"mask_ip\" ] } }","title":"PII Rule Redaction Methods"},{"location":"pii-config/methods/#pii-rule-redaction-methods","text":"","title":"PII Rule Redaction Methods"},{"location":"pii-config/methods/#remove","text":"Remove the entire field. Relay may choose to either set it to null or to remove it entirely. { \"rules\" : { \"remove_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"text\" : [ \"remove_ip\" ] } }","title":"remove"},{"location":"pii-config/methods/#replace","text":"Replace the key with a static string. { \"rules\" : { \"replace_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"replace\" , \"text\" : \"[censored]\" } } }, \"applications\" : { \"text\" : [ \"replace_ip\" ] } }","title":"replace"},{"location":"pii-config/methods/#mask","text":"Replace every character of the matched string with a \"masking\" char. Compared to replace this preserves the length of the original string. { \"rules\" : { \"mask_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"mask\" , \"mask_char\" : \"0\" , // The character used for masking. Optional, default \"*\" \"chars_to_ignore\" : \".\" , // Which characters to ignore. Optional, default empty \"range\" : [ 0 , - 1 ] // Which range of the string to replace. Optional, defaults to full range. Negative indices count from the matches' end. } } }, \"applications\" : { \"text\" : [ \"mask_ip\" ] } }","title":"mask"},{"location":"pii-config/methods/#hash","text":"Replace the string with a hashed version of itself. Equal strings will produce the same hash, so if you, for example, decide to hash the user ID instead of replacing or removing it, you will still have an accurate count of users affected. { \"rules\" : { \"hash_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"hash\" , \"algorithm\" : \"HMAC-SHA1\" , // One of \"HMAC-SHA1\", \"HMAC-SHA256\", \"HMAC-SHA512\" \"key\" : \"myOverriddenKey\" // A key to salt the hash with. Defaults to the default key set in \"vars\" } } }, \"vars\" : { \"hashKey\" : \"myDefaultKey\" // The default key to use } \"applications\" : { \"text\" : [ \"mask_ip\" ] } }","title":"hash"},{"location":"pii-config/types/","text":"PII Rule Types pattern Custom Perl-style regex (PCRE). { \"rules\" : { \"hash_device_id\" : { \"type\" : \"pattern\" , \"pattern\" : \"d/[a-f0-9]{12}\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_device_id\" ] } } imei Matches an IMEI or IMEISV. { \"rules\" : { \"hash_imei\" : { \"type\" : \"imei\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_imei\" ] } } mac Matches a MAC address. { \"rules\" : { \"hash_mac\" : { \"type\" : \"mac\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_mac\" ] } } ip Matches any IP address. { \"rules\" : { \"hash_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_ip\" ] } } creditcard Matches a creditcard number. { \"rules\" : { \"hash_cc\" : { \"type\" : \"creditcard\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_cc\" ] } } userpath Matches a local path (e.g. C:/Users/foo/ ). { \"rules\" : { \"hash_userpath\" : { \"type\" : \"userpath\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_userpath\" ] } } anything Matches any value. This is basically equivalent to a wildcard regex. For example, to remove all data with the PII kind text : { \"rules\" : { \"remove_everything\" : { \"type\" : \"anything\" , \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"text\" : [ \"remove_everything\" ] } } multiple Combine multiple rules into one. This is a disjunction (OR): The field in question has to match only one of the rules to match the combined rule, not all of them. { \"rules\" : { \"remove_ips_and_macs\" : { \"type\" : \"multiple\" , \"rules\" : [ \"@ip\" , \"@mac\" ], \"hide_rule\" : false , // Hide the inner rules when showing which rules have been applied. Defaults to false. \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"text\" : [ \"remove_ips_and_macs\" ] } } alias Alias one rule to the other. This is the same as multiple except that you can only wrap one rule. { \"rules\" : { \"remove_ips\" : { \"type\" : \"multiple\" , \"rule\" : \"@ip\" , \"hide_rule\" : false , // Hide the inner rule when showing which rules have been applied. Defaults to false. \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"text\" : [ \"remove_ips\" ] } } Apply a regex to the key name. The value matches if the key matches the given regex. This is useful for removing tokens and keys, since those are usually completely random. { \"applications\" : { \"container\" : [ \"remove_all_passwords\" ] }, \"rules\" : { \"remove_all_passwords\" : { \"keyPattern\" : \"(password|token|credentials)\" , \"type\" : \"redact_pair\" } } }","title":"PII Rule Types"},{"location":"pii-config/types/#pii-rule-types","text":"","title":"PII Rule Types"},{"location":"pii-config/types/#pattern","text":"Custom Perl-style regex (PCRE). { \"rules\" : { \"hash_device_id\" : { \"type\" : \"pattern\" , \"pattern\" : \"d/[a-f0-9]{12}\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_device_id\" ] } }","title":"pattern"},{"location":"pii-config/types/#imei","text":"Matches an IMEI or IMEISV. { \"rules\" : { \"hash_imei\" : { \"type\" : \"imei\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_imei\" ] } }","title":"imei"},{"location":"pii-config/types/#mac","text":"Matches a MAC address. { \"rules\" : { \"hash_mac\" : { \"type\" : \"mac\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_mac\" ] } }","title":"mac"},{"location":"pii-config/types/#ip","text":"Matches any IP address. { \"rules\" : { \"hash_ip\" : { \"type\" : \"ip\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_ip\" ] } }","title":"ip"},{"location":"pii-config/types/#creditcard","text":"Matches a creditcard number. { \"rules\" : { \"hash_cc\" : { \"type\" : \"creditcard\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_cc\" ] } }","title":"creditcard"},{"location":"pii-config/types/#userpath","text":"Matches a local path (e.g. C:/Users/foo/ ). { \"rules\" : { \"hash_userpath\" : { \"type\" : \"userpath\" , \"redaction\" : { \"method\" : \"hash\" } } }, \"applications\" : { \"text\" : [ \"hash_userpath\" ] } }","title":"userpath"},{"location":"pii-config/types/#anything","text":"Matches any value. This is basically equivalent to a wildcard regex. For example, to remove all data with the PII kind text : { \"rules\" : { \"remove_everything\" : { \"type\" : \"anything\" , \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"text\" : [ \"remove_everything\" ] } }","title":"anything"},{"location":"pii-config/types/#multiple","text":"Combine multiple rules into one. This is a disjunction (OR): The field in question has to match only one of the rules to match the combined rule, not all of them. { \"rules\" : { \"remove_ips_and_macs\" : { \"type\" : \"multiple\" , \"rules\" : [ \"@ip\" , \"@mac\" ], \"hide_rule\" : false , // Hide the inner rules when showing which rules have been applied. Defaults to false. \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"text\" : [ \"remove_ips_and_macs\" ] } }","title":"multiple"},{"location":"pii-config/types/#alias","text":"Alias one rule to the other. This is the same as multiple except that you can only wrap one rule. { \"rules\" : { \"remove_ips\" : { \"type\" : \"multiple\" , \"rule\" : \"@ip\" , \"hide_rule\" : false , // Hide the inner rule when showing which rules have been applied. Defaults to false. \"redaction\" : { \"method\" : \"remove\" } } }, \"applications\" : { \"text\" : [ \"remove_ips\" ] } } Apply a regex to the key name. The value matches if the key matches the given regex. This is useful for removing tokens and keys, since those are usually completely random. { \"applications\" : { \"container\" : [ \"remove_all_passwords\" ] }, \"rules\" : { \"remove_all_passwords\" : { \"keyPattern\" : \"(password|token|credentials)\" , \"type\" : \"redact_pair\" } } }","title":"alias"}]}